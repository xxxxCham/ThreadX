"""
Script de synchronisation INTELLIGENTE des donn√©es ThreadX
Utilise les donn√©es existantes (3m/5m/15m/1h) au lieu de tout re-t√©l√©charger
"""

import sys
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, Dict
import pandas as pd

# Ajouter src au PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from threadx.data.ingest import IngestionManager
from threadx.config import get_settings

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


class DownloadStats:
    """Compte les bougies t√©l√©charg√©es et calcule le taux par minute (fen√™tre glissante 60s)."""

    def __init__(self):
        from collections import deque
        import threading

        self.total = 0
        self._records = deque()  # tuples (timestamp, count)
        self._lock = threading.Lock()

    def add(self, count: int):
        """Ajouter `count` bougies t√©l√©charg√©es."""
        import time

        now = time.time()
        with self._lock:
            self.total += int(count)
            self._records.append((now, int(count)))
            # Nettoyer les enregistrements vieux de plus de 120s pour s√©curit√©
            cutoff = now - 120
            while self._records and self._records[0][0] < cutoff:
                self._records.popleft()

    def rate_per_minute(self) -> int:
        """Retourne le total de bougies ajout√©es dans la derni√®re minute."""
        import time

        now = time.time()
        cutoff = now - 60
        s = 0
        with self._lock:
            for ts, c in self._records:
                if ts >= cutoff:
                    s += c
        return int(s)


class SmartSyncManager:
    """Gestionnaire intelligent de synchronisation"""

    def __init__(self, settings):
        self.settings = settings
        self.manager = IngestionManager(settings)
        self.data_root = Path(settings.DATA_ROOT)
        # Statistiques de t√©l√©chargement
        self.stats = DownloadStats()

        # Timeframes disponibles (du plus au moins granulaire)
        self.available_timeframes = ["1h", "15m", "5m", "3m", "1m"]
        self.target_timeframes = ["1h", "4h", "1d"]

    def find_existing_data(self, symbol: str) -> Optional[str]:
        """Trouve le meilleur timeframe existant pour ce symbole"""

        logger.info(f"üîç Recherche donn√©es existantes pour {symbol}...")

        # Chercher dans l'ordre de priorit√©
        for tf in self.available_timeframes:
            # V√©rifier dans processed/
            processed_path = self.data_root / "processed" / symbol / f"{tf}.parquet"

            if processed_path.exists():
                try:
                    df = pd.read_parquet(processed_path)
                    if not df.empty and len(df) > 100:
                        logger.info(
                            f"‚úÖ Trouv√©: {tf} ({len(df):,} barres) "
                            f"dans {processed_path.parent.name}"
                        )
                        return tf
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Erreur lecture {tf}: {e}")
                    continue

            # V√©rifier dans raw/
            raw_path = self.data_root / "raw" / tf / f"{symbol}.parquet"
            if raw_path.exists():
                try:
                    df = pd.read_parquet(raw_path)
                    if not df.empty and len(df) > 100:
                        logger.info(
                            f"‚úÖ Trouv√©: {tf} ({len(df):,} barres) " f"dans raw/{tf}"
                        )
                        return tf
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Erreur lecture {tf}: {e}")
                    continue

        logger.warning(f"‚ö†Ô∏è  Aucune donn√©e existante pour {symbol}")
        return None

    def check_data_completeness(
        self, symbol: str, timeframe: str, start_date: datetime, end_date: datetime
    ) -> Dict:
        """V√©rifie si les donn√©es sont compl√®tes pour la p√©riode"""

        try:
            # Charger les donn√©es
            df = self.load_data(symbol, timeframe)

            if df.empty:
                return {"complete": False, "coverage": 0.0, "bars": 0, "gaps": []}

            # Calculer la couverture
            total_days = (end_date - start_date).days
            data_start = df.index.min()
            data_end = df.index.max()

            # Convertir les dates en timezone-aware si n√©cessaire
            if data_start.tzinfo is not None:
                # Les donn√©es ont une timezone, convertir start/end en UTC
                import pytz

                if start_date.tzinfo is None:
                    start_date = pytz.UTC.localize(start_date)
                if end_date.tzinfo is None:
                    end_date = pytz.UTC.localize(end_date)
            else:
                # Les donn√©es n'ont pas de timezone, retirer la timezone de start/end
                if start_date.tzinfo is not None:
                    start_date = start_date.replace(tzinfo=None)
                if end_date.tzinfo is not None:
                    end_date = end_date.replace(tzinfo=None)

            # V√©rifier si la plage est couverte
            coverage_start = data_start <= start_date
            coverage_end = data_end >= end_date - timedelta(days=2)

            expected = self.calculate_expected_bars(timeframe, start_date, end_date)
            coverage_pct = len(df) / expected if total_days > 0 and expected > 0 else 0
            missing = max(0, expected - len(df)) if expected > 0 else 0

            is_complete = coverage_start and coverage_end and coverage_pct >= 0.90

            return {
                "complete": is_complete,
                "coverage": coverage_pct,
                "bars": len(df),
                "expected": expected,
                "missing": missing,
                "data_start": data_start,
                "data_end": data_end,
                "gaps": [],  # √Ä impl√©menter si besoin
            }

        except Exception as e:
            logger.error(f"‚ùå Erreur v√©rification compl√©tude: {e}")
            import traceback

            logger.error(traceback.format_exc())
            return {"complete": False, "coverage": 0.0, "bars": 0}

    def load_data(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """Charge les donn√©es depuis processed/ ou raw/"""

        # Essayer processed/ d'abord
        processed_path = self.data_root / "processed" / symbol / f"{timeframe}.parquet"
        if processed_path.exists():
            return pd.read_parquet(processed_path)

        # Essayer raw/
        raw_path = self.data_root / "raw" / timeframe / f"{symbol}.parquet"
        if raw_path.exists():
            return pd.read_parquet(raw_path)

        return pd.DataFrame()

    def calculate_expected_bars(
        self, timeframe: str, start: datetime, end: datetime
    ) -> int:
        """Calcule le nombre de barres attendu"""

        total_hours = (end - start).total_seconds() / 3600

        bars_per_hour = {
            "1m": 60,
            "3m": 20,
            "5m": 12,
            "15m": 4,
            "1h": 1,
            "4h": 0.25,
            "1d": 1 / 24,
        }

        return int(total_hours * bars_per_hour.get(timeframe, 0))

    def find_missing_ranges(
        self, df: pd.DataFrame, timeframe: str, start: datetime, end: datetime
    ):
        """Retourne une liste de tuples (start, end) des plages manquantes pour le timeframe donn√©.

        Se base sur un DateRange attendu et compare √† l'index pr√©sent.
        """
        try:
            if df is None or df.empty:
                return [(start, end)]

            freq_map = {
                "1m": "1min",
                "3m": "3min",
                "5m": "5min",
                "15m": "15min",
                "1h": "1h",
                "4h": "4h",
                "1d": "1d",
            }
            freq = freq_map.get(timeframe, "1min")

            # Normaliser timezone comme dans check_data_completeness
            data_tz = df.index.tz
            try:
                import pytz

                if data_tz is not None:
                    if start.tzinfo is None:
                        start = pytz.UTC.localize(start)
                    if end.tzinfo is None:
                        end = pytz.UTC.localize(end)
                else:
                    if start.tzinfo is not None:
                        start = start.replace(tzinfo=None)
                    if end.tzinfo is not None:
                        end = end.replace(tzinfo=None)
            except Exception:
                # Si pytz indisponible, continuer sans timezone
                pass

            expected = pd.date_range(start=start, end=end, freq=freq)
            present = pd.Index(df.index)
            missing = expected.difference(present)
            if missing.empty:
                return []

            # Grouper en plages contigu√´s
            ranges = []
            cur_start = missing[0]
            prev = missing[0]
            td = pd.Timedelta(freq)
            for ts in missing[1:]:
                if (ts - prev) > td:
                    ranges.append((cur_start.to_pydatetime(), prev.to_pydatetime()))
                    cur_start = ts
                prev = ts

            ranges.append((cur_start.to_pydatetime(), prev.to_pydatetime()))
            return ranges
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur d√©termination plages manquantes: {e}")
            return []

    def sync_symbol_smart(
        self, symbol: str, start_date: datetime, end_date: datetime
    ) -> bool:
        """Synchronisation intelligente d'un symbole"""

        logger.info("=" * 60)
        logger.info(f"üöÄ Synchronisation intelligente: {symbol}")
        logger.info("=" * 60)

        # 1. Trouver les donn√©es existantes
        best_tf = self.find_existing_data(symbol)

        if best_tf:
            # 2. V√©rifier la compl√©tude
            check = self.check_data_completeness(symbol, best_tf, start_date, end_date)

            logger.info(
                f"üìä Compl√©tude {best_tf}: "
                f"{check['coverage']*100:.1f}% "
                f"({check['bars']:,} barres) "
                f"‚Äî manquantes: {check.get('missing', 0):,}"
            )

            if check["complete"]:
                logger.info(
                    f"‚úÖ Donn√©es {best_tf} suffisantes, "
                    f"g√©n√©ration des timeframes sup√©rieurs"
                )

                # Si des bougies manquent encore, t√©l√©charger les segments manquants (cas 1m principalement)
                if check.get("missing", 0) > 0:
                    logger.info(
                        f"‚¨áÔ∏è  {check['missing']:,} bougies manquantes d√©tect√©es: t√©l√©chargement des segments manquants..."
                    )

                    # Charger les donn√©es existantes
                    df_exist = self.load_data(symbol, best_tf)
                    ranges = self.find_missing_ranges(
                        df_exist, best_tf, start_date, end_date
                    )

                    if ranges:
                        for rstart, rend in ranges:
                            logger.info(f"‚¨áÔ∏è  Segment: {rstart} ‚Üí {rend}")
                            try:
                                seg = self.manager.download_ohlcv_1m(
                                    symbol=symbol, start=rstart, end=rend, force=True
                                )
                                if seg is not None and not seg.empty:
                                    self.stats.add(len(seg))
                                    logger.info(
                                        f"‚úÖ Segment t√©l√©charg√©: {len(seg):,} bougies"
                                    )
                                else:
                                    logger.warning(
                                        "‚ö†Ô∏è  Aucun segment t√©l√©charg√© pour cette plage"
                                    )
                            except Exception as e:
                                logger.error(f"‚ùå Erreur t√©l√©chargement segment: {e}")

                        # Apr√®s avoir combl√© les segments, rejouer la g√©n√©ration
                        logger.info(
                            "üîÅ Re-v√©rification apr√®s t√©l√©chargement des segments"
                        )
                        check2 = self.check_data_completeness(
                            symbol, best_tf, start_date, end_date
                        )
                        if not check2.get("complete", False):
                            logger.warning(
                                "‚ö†Ô∏è  Apr√®s t√©l√©chargement, donn√©es toujours incompl√®tes ‚Äî fallback sur t√©l√©chargement complet"
                            )
                            return self.download_and_resample(
                                symbol, start_date, end_date
                            )

                    # Charger √† nouveau et g√©n√©rer
                    return self.generate_from_existing(
                        symbol, best_tf, start_date, end_date
                    )
                # 3. G√©n√©rer les timeframes manquants depuis les donn√©es existantes
                return self.generate_from_existing(
                    symbol, best_tf, start_date, end_date
                )
            else:
                logger.warning(
                    f"‚ö†Ô∏è  Donn√©es {best_tf} incompl√®tes "
                    f"({check['coverage']*100:.1f}%), "
                    f"fallback sur t√©l√©chargement"
                )

        # 4. Fallback: t√©l√©charger depuis 1m
        logger.info("üì• T√©l√©chargement 1m n√©cessaire")
        return self.download_and_resample(symbol, start_date, end_date)

    def generate_from_existing(
        self, symbol: str, source_tf: str, start_date: datetime, end_date: datetime
    ) -> bool:
        """G√©n√®re les timeframes cibles depuis un timeframe existant"""

        success_count = 0

        # Charger les donn√©es source (1m dans la plupart des cas)
        logger.info(f"üìÇ Chargement donn√©es {source_tf} pour {symbol}")
        df_source = self.load_data(symbol, source_tf)

        if df_source.empty:
            logger.error(f"‚ùå Impossible de charger les donn√©es {source_tf}")
            return False

        logger.info(f"‚úÖ Charg√©: {len(df_source):,} barres {source_tf}")

        for target_tf in self.target_timeframes:
            # Ne pas r√©g√©n√©rer le m√™me timeframe
            if source_tf == target_tf:
                logger.info(f"‚è≠Ô∏è  {target_tf} d√©j√† disponible")
                success_count += 1
                continue

            # Ne g√©n√©rer que les timeframes sup√©rieurs
            tf_hierarchy = {
                "1m": 0,
                "3m": 1,
                "5m": 2,
                "15m": 3,
                "1h": 4,
                "4h": 5,
                "1d": 6,
            }

            if tf_hierarchy.get(source_tf, 0) > tf_hierarchy.get(target_tf, 0):
                logger.warning(
                    f"‚ö†Ô∏è  Impossible de g√©n√©rer {target_tf} "
                    f"depuis {source_tf} (timeframe plus petit)"
                )
                continue

            try:
                logger.info(f"üîÑ G√©n√©ration {target_tf} depuis {source_tf}")

                # Resample depuis le DataFrame source
                # Si source_tf == "1m", utiliser resample_from_1m
                if source_tf == "1m":
                    df_target = self.manager.resample_from_1m(
                        df_1m=df_source, timeframe=target_tf
                    )
                else:
                    # Pour les autres timeframes, utiliser la fonction g√©n√©rique
                    from threadx.data.resample import resample_ohlcv

                    df_target = resample_ohlcv(
                        df=df_source, source_tf=source_tf, target_tf=target_tf
                    )

                if not df_target.empty:
                    logger.info(f"‚úÖ G√©n√©r√©: {len(df_target):,} barres {target_tf}")
                    # Ces barres sont g√©n√©r√©es localement (pas de t√©l√©chargement)
                    success_count += 1
                else:
                    logger.warning(f"‚ö†Ô∏è  √âchec g√©n√©ration {target_tf}")

            except Exception as e:
                logger.error(f"‚ùå Erreur g√©n√©ration {target_tf}: {e}")
                import traceback

                logger.error(traceback.format_exc())

        return success_count >= len(self.target_timeframes) - 1

    def download_and_resample(
        self, symbol: str, start_date: datetime, end_date: datetime
    ) -> bool:
        """T√©l√©charge 1m et g√©n√®re tous les timeframes"""

        try:
            # T√©l√©charger 1m
            logger.info(f"‚¨áÔ∏è  T√©l√©chargement 1m pour {symbol}")
            df_1m = self.manager.download_ohlcv_1m(
                symbol=symbol, start=start_date, end=end_date, force=False
            )

            if df_1m.empty:
                logger.error(f"‚ùå Aucune donn√©e 1m re√ßue pour {symbol}")
                return False

            logger.info(f"‚úÖ T√©l√©charg√©: {len(df_1m):,} barres 1m")

            # Mettre √† jour les statistiques de t√©l√©chargement
            try:
                self.stats.add(len(df_1m))
                logger.info(
                    f"üìà Statistiques: {self.stats.rate_per_minute():,} bougies/min ‚Äî total t√©l√©charg√©es: {self.stats.total:,}"
                )
            except Exception:
                # Ne doit pas interrompre le flux principal
                logger.debug(
                    "‚ö†Ô∏è  Impossible de mettre √† jour les stats de t√©l√©chargement"
                )

            # G√©n√©rer tous les timeframes cibles
            success_count = 0
            for target_tf in self.target_timeframes:
                if target_tf == "1m":
                    success_count += 1
                    continue

                try:
                    logger.info(f"üîÑ Resample 1m ‚Üí {target_tf}")
                    # Resample depuis le DataFrame 1m t√©l√©charg√©
                    df_tf = self.manager.resample_from_1m(
                        df_1m=df_1m, timeframe=target_tf
                    )

                    if not df_tf.empty:
                        logger.info(f"‚úÖ Resampled: {len(df_tf):,} barres {target_tf}")
                        success_count += 1
                    else:
                        logger.warning(f"‚ö†Ô∏è  √âchec resample {target_tf}")

                except Exception as e:
                    logger.error(f"‚ùå Erreur resample {target_tf}: {e}")

            return success_count >= len(self.target_timeframes) - 1

        except Exception as e:
            logger.error(f"‚ùå Erreur t√©l√©chargement {symbol}: {e}")
            import traceback

            logger.error(traceback.format_exc())
            return False


def main(test_mode: bool = True):
    """Point d'entr√©e principal"""

    logger.info("=" * 80)
    logger.info("üöÄ SYNCHRONISATION INTELLIGENTE ThreadX")
    logger.info("Utilise les donn√©es existantes avant de t√©l√©charger")
    logger.info("=" * 80)

    # Configuration
    settings = get_settings()
    smart_sync = SmartSyncManager(settings)

    # P√©riode
    start_date = datetime(2025, 1, 1)
    end_date = datetime.now() - timedelta(days=1)
    end_date = end_date.replace(hour=23, minute=59, second=59)

    logger.info(f"üìÖ P√©riode: {start_date.date()} ‚Üí {end_date.date()}")

    # Symboles
    all_symbols = [
        "BTCUSDC",
        "ETHUSDC",
        "BNBUSDC",
        "SOLUSDC",
        "XRPUSDC",
        "ADAUSDC",
        "AVAXUSDC",
        "DOTUSDC",
    ]

    # Mode test: un seul symbole
    if test_mode:
        symbols = ["BTCUSDC"]
        logger.info("üß™ MODE TEST: BTCUSDC uniquement")
    else:
        symbols = all_symbols
        logger.info(f"üéØ {len(symbols)} symboles √† synchroniser")

    logger.info(f"‚è±Ô∏è  Timeframes cibles: {', '.join(smart_sync.target_timeframes)}")
    logger.info("")

    # Synchroniser
    success = 0
    failed = 0

    for i, symbol in enumerate(symbols, 1):
        logger.info("")
        logger.info(f"üìä Symbole {i}/{len(symbols)}: {symbol}")

        try:
            if smart_sync.sync_symbol_smart(symbol, start_date, end_date):
                success += 1
                logger.info(f"‚úÖ {symbol} synchronis√© avec succ√®s")
            else:
                failed += 1
                logger.error(f"‚ùå √âchec synchronisation {symbol}")
        except Exception as e:
            failed += 1
            logger.error(f"‚ùå Erreur {symbol}: {e}")
            import traceback

            logger.error(traceback.format_exc())

    # R√©sum√©
    logger.info("")
    logger.info("=" * 80)
    logger.info("üìä R√âSUM√â SYNCHRONISATION INTELLIGENTE")
    logger.info("=" * 80)
    logger.info(f"‚úÖ R√©ussies: {success}/{len(symbols)}")
    logger.info(f"‚ùå √âchou√©es: {failed}/{len(symbols)}")
    logger.info(f"üìÖ P√©riode: {start_date.date()} ‚Üí {end_date.date()}")

    # Afficher statistiques de t√©l√©chargement
    try:
        logger.info("")
        logger.info("üìà Statistiques de t√©l√©chargement globales")
        logger.info(f"üî¢ Total bougies t√©l√©charg√©es: {smart_sync.stats.total:,}")
        logger.info(
            f"‚ö° Taux actuel (60s): {smart_sync.stats.rate_per_minute():,} bougies/min"
        )
    except Exception:
        logger.debug("‚ö†Ô∏è  Impossible d'afficher les statistiques de t√©l√©chargement")

    if test_mode:
        logger.info("")
        logger.info("üß™ Test termin√© avec BTCUSDC")
        logger.info(
            "üí° Pour lancer tous les symboles: python scripts/sync_data_smart.py --full"
        )

    logger.info("=" * 80)
    logger.info("üéâ SYNCHRONISATION TERMIN√âE")
    logger.info("=" * 80)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--full",
        action="store_true",
        help="Synchroniser tous les symboles (sinon test avec BTCUSDC)",
    )

    args = parser.parse_args()

    try:
        main(test_mode=not args.full)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è  Interruption utilisateur")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {e}")
        import traceback

        logger.error(traceback.format_exc())
        sys.exit(1)
