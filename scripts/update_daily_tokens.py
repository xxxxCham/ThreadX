#!/usr/bin/env python3
"""
Mise √† jour quotidienne des top tokens et leurs donn√©es OHLCV.
√Ä ex√©cuter chaque matin avant le d√©but du trading.

Usage:
    python update_daily_tokens.py

    Options:
        --tokens 150        Nombre de tokens (d√©faut: 100)
        --timeframes 1h,4h  Timeframes √† mettre √† jour (d√©faut: 1h,4h)
        --days 365          Jours d'historique (d√©faut: 365)
"""

import sys
from pathlib import Path
from datetime import datetime
import argparse

# Import modules consolid√©s (direct import pour √©viter d√©pendances config)
sys.path.insert(0, str(Path(__file__).parent))

import importlib.util

# Import TokenManager
spec = importlib.util.spec_from_file_location(
    "tokens", Path(__file__).parent / "src" / "threadx" / "data" / "tokens.py"
)
tokens_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(tokens_module)
TokenManager = tokens_module.TokenManager

# Import BinanceDataLoader
spec = importlib.util.spec_from_file_location(
    "loader", Path(__file__).parent / "src" / "threadx" / "data" / "loader.py"
)
loader_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(loader_module)
BinanceDataLoader = loader_module.BinanceDataLoader


def update_daily_tokens(num_tokens=100, timeframes=None, days_history=365):
    """
    Mise √† jour quotidienne compl√®te des tokens.

    Args:
        num_tokens: Nombre de tokens √† r√©cup√©rer
        timeframes: Liste des timeframes √† mettre √† jour
        days_history: Nombre de jours d'historique
    """

    if timeframes is None:
        timeframes = ["1h", "4h"]

    print("=" * 70)
    print(
        f"üìÖ MISE √Ä JOUR QUOTIDIENNE - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
    )
    print("=" * 70)

    # √âTAPE 1: Mise √† jour liste tokens top N
    print(f"\nüìù √âTAPE 1/3: R√©cup√©ration top {num_tokens} tokens...")
    print("-" * 70)

    token_mgr = TokenManager(
        cache_path=Path("data/crypto_data_json/tokens_top100.json")
    )

    # R√©cup√©rer top N avec donn√©es fra√Æches
    top_tokens = token_mgr.get_top_tokens(
        limit=num_tokens,
        usdc_only=True,
        force_refresh=True,  # Force rafra√Æchissement quotidien
    )

    print(f"‚úÖ {len(top_tokens)} tokens USDC s√©lectionn√©s")
    print(f"   Top 10: {', '.join(top_tokens[:10])}")

    # √âTAPE 2: T√©l√©chargement donn√©es OHLCV
    print(f"\n\nüì• √âTAPE 2/3: T√©l√©chargement OHLCV ({days_history} jours)...")
    print("-" * 70)

    loader = BinanceDataLoader(
        json_cache_dir=Path("data/crypto_data_json"),
        parquet_cache_dir=Path("data/crypto_data_parquet"),
    )

    total_downloaded = 0
    total_failed = 0

    for tf in timeframes:
        print(f"\n‚è±Ô∏è  Timeframe: {tf}")

        # Callback progression
        def progress_callback(pct, done, total):
            if done % 20 == 0 or done == total:
                print(f"   [{done}/{total}] {pct:.1f}%")

        # T√©l√©chargement parall√®le
        results = loader.download_multiple(
            symbols=top_tokens,
            interval=tf,
            days_history=days_history,
            max_workers=4,
            progress_callback=progress_callback,
        )

        downloaded = len(results)
        failed = len(top_tokens) - downloaded
        total_downloaded += downloaded
        total_failed += failed

        print(f"   ‚úÖ {downloaded}/{len(top_tokens)} t√©l√©charg√©s")
        if failed > 0:
            print(f"   ‚ö†Ô∏è  {failed} √©checs")

    # √âTAPE 3: R√©sum√© et statistiques
    print(f"\n\nüìä √âTAPE 3/3: R√©sum√©")
    print("-" * 70)

    # Statistiques stockage
    json_dir = Path("data/crypto_data_json")
    parquet_dir = Path("data/crypto_data_parquet")

    json_files = len(list(json_dir.glob("*.json"))) if json_dir.exists() else 0
    parquet_files = (
        len(list(parquet_dir.glob("*.parquet"))) if parquet_dir.exists() else 0
    )

    # Taille fichiers
    def get_dir_size(directory):
        total = 0
        for file in Path(directory).rglob("*"):
            if file.is_file():
                total += file.stat().st_size
        return total / (1024 * 1024)  # MB

    json_size = get_dir_size(json_dir) if json_dir.exists() else 0
    parquet_size = get_dir_size(parquet_dir) if parquet_dir.exists() else 0

    print(f"üìÅ Tokens:           {len(top_tokens)}")
    print(f"üìÅ Timeframes:       {', '.join(timeframes)}")
    print(f"üìÅ Fichiers JSON:    {json_files} ({json_size:.1f} MB)")
    print(f"üìÅ Fichiers Parquet: {parquet_files} ({parquet_size:.1f} MB)")
    print(f"üì• T√©l√©charg√©s:      {total_downloaded}")
    if total_failed > 0:
        print(f"‚ö†Ô∏è  √âchecs:          {total_failed}")

    print(f"\n‚úÖ Mise √† jour quotidienne termin√©e!")

    return {
        "tokens_count": len(top_tokens),
        "timeframes": timeframes,
        "downloaded": total_downloaded,
        "failed": total_failed,
        "json_files": json_files,
        "parquet_files": parquet_files,
    }


def main():
    parser = argparse.ArgumentParser(
        description="Mise √† jour quotidienne top tokens + OHLCV"
    )
    parser.add_argument(
        "--tokens", type=int, default=100, help="Nombre de tokens (d√©faut: 100)"
    )
    parser.add_argument(
        "--timeframes",
        type=str,
        default="1h,4h",
        help="Timeframes s√©par√©s par virgule (d√©faut: 1h,4h)",
    )
    parser.add_argument(
        "--days", type=int, default=365, help="Jours d'historique (d√©faut: 365)"
    )

    args = parser.parse_args()

    timeframes = [tf.strip() for tf in args.timeframes.split(",")]

    stats = update_daily_tokens(
        num_tokens=args.tokens, timeframes=timeframes, days_history=args.days
    )

    print("\n" + "=" * 70)
    print("‚úÖ MISE √Ä JOUR QUOTIDIENNE COMPL√àTE")
    print("=" * 70)


if __name__ == "__main__":
    main()
