#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TradXPro Core Manager - Module Unifi√©
=====================================

Module tout-en-un qui incorpore toute la logique TradXPro :
- Gestion des t√©l√©chargements crypto
- S√©lection des tokens (top 100 marketcap/volume)
- Chargement et traitement des donn√©es OHLCV
- Calcul et cache des indicateurs techniques
- Gestion des fichiers JSON/Parquet
- API simplifi√©e pour int√©gration

Utilisation :
    from tradxpro_core_manager import TradXProManager
    
    manager = TradXProManager()
    
    # R√©cup√©rer les 100 meilleurs tokens
    top_tokens = manager.get_top_100_tokens()
    
    # T√©l√©charger les donn√©es
    manager.download_crypto_data(["BTCUSDC", "ETHUSDC"])
    
    # Charger avec indicateurs
    df = manager.get_trading_data("BTCUSDC", "1h", indicators=["rsi", "bollinger"])

Auteur: TradXPro Team
Date: 2 octobre 2025
Version: 1.0
"""

import os
import sys
import json
import time
import logging
import platform
import requests
from pathlib import Path
from typing import List, Dict, Optional, Any, Callable, Tuple, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta

import pandas as pd
import numpy as np

# Configuration des chemins TradXPro
IS_WINDOWS = platform.system() == "Windows"

class TradXProPaths:
    """Gestionnaire centralis√© des chemins TradXPro"""
    
    def __init__(self, root_path: Optional[str] = None):
        if root_path is None:
            self.root = Path(r"D:\TradXPro") if IS_WINDOWS else Path("/home/user/TradXPro")
        else:
            self.root = Path(root_path)
        
        # Dossiers principaux
        self.json_root = self.root / "crypto_data_json"
        self.parquet_root = self.root / "crypto_data_parquet"
        self.indicators_db = self.root / "indicators_db"
        self.scripts_dir = self.root / "scripts" / "mise_a_jour_dataframe"
        
        # Fichiers de configuration
        self.tokens_json = self.scripts_dir / "resultats_choix_des_100tokens.json"
        self.log_file = self.scripts_dir / "unified_data_historique.log"
        
        # Cr√©ation des dossiers si n√©cessaire
        self._ensure_directories()
    
    def _ensure_directories(self):
        """Cr√©e les dossiers n√©cessaires s'ils n'existent pas"""
        for path in [self.json_root, self.parquet_root, self.indicators_db, self.scripts_dir]:
            path.mkdir(parents=True, exist_ok=True)

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class TradXProManager:
    """
    Gestionnaire principal TradXPro - API unifi√©e pour toutes les fonctionnalit√©s
    """
    
    def __init__(self, root_path: Optional[str] = None):
        """
        Initialise le gestionnaire TradXPro
        
        Args:
            root_path: Chemin racine personnalis√© (optionnel)
        """
        self.paths = TradXProPaths(root_path)
        self.logger = logger
        
        # Configuration par d√©faut
        self.history_days = 365
        self.binance_limit = 1000
        self.intervals = ["3m", "5m", "15m", "30m", "1h"]
        self.max_workers = max(4, (os.cpu_count() or 8) // 2)
        
        logger.info(f"TradXPro Manager initialis√© - Racine: {self.paths.root}")
    
    # =========================================================
    #  SECTION 1: Gestion des tokens (Top 100)
    # =========================================================
    
    def get_top_100_marketcap_coingecko(self) -> List[Dict]:
        """
        R√©cup√®re les 100 cryptos avec la plus grosse capitalisation via CoinGecko
        
        Returns:
            Liste des tokens avec marketcap, nom, symbol, rang
        """
        url = "https://api.coingecko.com/api/v3/coins/markets"
        params = {
            "vs_currency": "usd",
            "order": "market_cap_desc",
            "per_page": 100,
            "page": 1
        }
        
        try:
            logger.info("R√©cup√©ration top 100 marketcap CoinGecko...")
            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            result = []
            for entry in data:
                result.append({
                    "symbol": entry["symbol"].upper(),
                    "name": entry["name"],
                    "market_cap": entry.get("market_cap", 0),
                    "market_cap_rank": entry.get("market_cap_rank", 999),
                    "volume": entry.get("total_volume", 0)
                })
            
            logger.info(f"‚úÖ {len(result)} tokens r√©cup√©r√©s via CoinGecko")
            return result
            
        except Exception as e:
            logger.error(f"Erreur CoinGecko API: {e}")
            return []
    
    def get_top_100_volume_binance(self) -> List[Dict]:
        """
        R√©cup√®re les 100 cryptos USDC avec le plus gros volume 24h via Binance
        
        Returns:
            Liste des tokens USDC avec volume 24h
        """
        url = "https://api.binance.com/api/v3/ticker/24hr"
        
        try:
            logger.info("R√©cup√©ration top 100 volume USDC Binance...")
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            data = response.json()
            
            # Filtrer uniquement les paires USDC
            usdc_pairs = []
            for entry in data:
                if entry["symbol"].endswith("USDC"):
                    base_asset = entry["symbol"].replace("USDC", "")
                    usdc_pairs.append({
                        "symbol": base_asset,
                        "volume": float(entry["quoteVolume"]) if entry["quoteVolume"] else 0,
                        "price_change": float(entry["priceChangePercent"]) if entry["priceChangePercent"] else 0
                    })
            
            # Trier par volume d√©croissant et prendre les 100 premiers
            usdc_pairs.sort(key=lambda x: x["volume"], reverse=True)
            result = usdc_pairs[:100]
            
            logger.info(f"‚úÖ {len(result)} tokens USDC r√©cup√©r√©s via Binance")
            return result
            
        except Exception as e:
            logger.error(f"Erreur Binance API: {e}")
            return []
    
    def _ensure_category_representation(self, tokens: List[Dict]) -> List[Dict]:
        """
        Garantit qu'au moins les 3 meilleures cryptos de chaque cat√©gorie importante sont incluses
        
        Args:
            tokens: Liste des tokens tri√©s par score
            
        Returns:
            Liste ajust√©e avec repr√©sentation garantie par cat√©gorie
        """
        # D√©finition des cat√©gories importantes avec leurs tokens repr√©sentatifs
        essential_categories = {
            "layer1_blockchain": ["BTC", "ETH", "ADA", "SOL", "AVAX", "DOT", "NEAR", "ALGO"],
            "defi_protocols": ["UNI", "AAVE", "COMP", "MKR", "SUSHI", "CRV", "1INCH", "YFI"],
            "layer2_scaling": ["MATIC", "ARB", "OP", "IMX", "LRC", "MINA"],
            "smart_contracts": ["ETH", "ADA", "SOL", "AVAX", "DOT", "ALGO", "NEAR", "ATOM"],
            "meme_coins": ["DOGE", "SHIB", "PEPE", "FLOKI", "BONK"],
            "exchange_tokens": ["BNB", "CRO", "FTT", "HT", "KCS", "OKB"],
            "stablecoins": ["USDT", "USDC", "BUSD", "DAI", "FRAX", "TUSD"],
            "ai_gaming": ["FET", "AGIX", "OCEAN", "AXS", "SAND", "MANA", "ENJ"],
            "privacy_coins": ["XMR", "ZEC", "DASH", "SCRT"],
            "infrastructure": ["LINK", "GRT", "FIL", "AR", "STORJ", "SIA"]
        }
        
        logger.info("üîç V√©rification de la repr√©sentation par cat√©gorie...")
        
        # Cr√©er un index des tokens actuels
        current_symbols = {token["symbol"] for token in tokens}
        guaranteed_tokens = []
        
        # Pour chaque cat√©gorie, garantir au moins 3 tokens du top marketcap
        for category, category_tokens in essential_categories.items():
            category_count = 0
            category_found = []
            
            # V√©rifier les tokens d√©j√† pr√©sents dans cette cat√©gorie
            for token in tokens:
                if token["symbol"] in category_tokens:
                    category_found.append(token)
                    category_count += 1
            
            # Si moins de 3 tokens de cette cat√©gorie, essayer d'en ajouter
            if category_count < 3:
                missing_count = 3 - category_count
                logger.debug(f"Cat√©gorie {category}: {category_count} tokens pr√©sents, besoin de {missing_count} suppl√©mentaires")
                
                # Chercher les tokens manquants dans les donn√©es originales
                for symbol in category_tokens:
                    if symbol not in current_symbols and missing_count > 0:
                        # Cr√©er un token de base avec score √©lev√© pour garantir l'inclusion
                        guaranteed_token = {
                            "symbol": symbol,
                            "name": symbol,
                            "market_cap": 0,
                            "market_cap_rank": 999,
                            "volume": 0,
                            "price_change": 0,
                            "source": "category_guarantee",
                            "category": category,
                            "score": 150  # Score √©lev√© pour garantir l'inclusion
                        }
                        guaranteed_tokens.append(guaranteed_token)
                        current_symbols.add(symbol)
                        missing_count -= 1
                        logger.debug(f"Token {symbol} ajout√© pour garantir la cat√©gorie {category}")
        
        # Fusionner les tokens garantis avec la liste originale
        if guaranteed_tokens:
            combined_tokens = tokens + guaranteed_tokens
            # Retrier par score
            combined_tokens.sort(key=lambda x: x["score"], reverse=True)
            logger.info(f"‚úÖ {len(guaranteed_tokens)} tokens ajout√©s pour garantir la diversit√© des cat√©gories")
            return combined_tokens[:100]  # Toujours retourner 100 tokens max
        
        return tokens

    def merge_and_select_top_100(self, marketcap_list: List[Dict], volume_list: List[Dict]) -> List[Dict]:
        """
        Fusionne les listes marketcap et volume pour s√©lectionner les 100 meilleurs tokens
        avec garantie de repr√©sentation par cat√©gorie
        
        Args:
            marketcap_list: Liste des tokens par marketcap
            volume_list: Liste des tokens par volume
            
        Returns:
            Liste fusionn√©e des 100 meilleurs tokens avec repr√©sentation garantie
        """
        logger.info("Fusion des listes marketcap et volume avec garantie de diversit√©...")
        
        # Index par symbole
        marketcap_dict = {token["symbol"]: token for token in marketcap_list}
        volume_dict = {token["symbol"]: token for token in volume_list}
        
        # Fusion des donn√©es
        merged_tokens = {}
        all_symbols = set(marketcap_dict.keys()) | set(volume_dict.keys())
        
        for symbol in all_symbols:
            mc_data = marketcap_dict.get(symbol, {})
            vol_data = volume_dict.get(symbol, {})
            
            merged_tokens[symbol] = {
                "symbol": symbol,
                "name": mc_data.get("name", symbol),
                "market_cap": mc_data.get("market_cap", 0),
                "market_cap_rank": mc_data.get("market_cap_rank", 999),
                "volume": vol_data.get("volume", 0),
                "price_change": vol_data.get("price_change", 0),
                "source": "both" if (symbol in marketcap_dict and symbol in volume_dict) else 
                         ("marketcap" if symbol in marketcap_dict else "volume")
            }
        
        # Scoring composite pour s√©lectionner les meilleurs
        scored_tokens = []
        for token in merged_tokens.values():
            # Score bas√© sur marketcap (invers√© car rang 1 = meilleur) et volume
            mc_score = max(0, 101 - token["market_cap_rank"]) if token["market_cap_rank"] < 999 else 0
            vol_score = min(100, token["volume"] / 1_000_000)  # Normalisation volume
            
            # Bonus si pr√©sent dans les deux listes
            bonus = 20 if token["source"] == "both" else 0
            
            total_score = mc_score + vol_score + bonus
            token["score"] = total_score
            scored_tokens.append(token)
        
        # Trier par score d√©croissant
        scored_tokens.sort(key=lambda x: x["score"], reverse=True)
        
        # Appliquer la garantie de repr√©sentation par cat√©gorie
        diversified_tokens = self._ensure_category_representation(scored_tokens)
        
        # Prendre les 100 premiers apr√®s diversification
        top_100 = diversified_tokens[:100]
        
        # Statistiques finales
        avg_score = np.mean([t['score'] for t in top_100])
        category_stats = {}
        for token in top_100:
            source = token.get("source", "unknown")
            category_stats[source] = category_stats.get(source, 0) + 1
        
        logger.info(f"‚úÖ Top 100 tokens s√©lectionn√©s avec diversit√© garantie:")
        logger.info(f"   Score moyen: {avg_score:.1f}")
        logger.info(f"   R√©partition: {category_stats}")
        
        return top_100
    
    def analyze_token_diversity(self, tokens: List[Dict]) -> Dict[str, Any]:
        """
        Analyse la diversit√© des tokens s√©lectionn√©s par cat√©gorie
        
        Args:
            tokens: Liste des tokens √† analyser
            
        Returns:
            Dictionnaire avec statistiques de diversit√©
        """
        # D√©finition des cat√©gories (m√™me que dans _ensure_category_representation)
        categories = {
            "layer1_blockchain": ["BTC", "ETH", "ADA", "SOL", "AVAX", "DOT", "NEAR", "ALGO"],
            "defi_protocols": ["UNI", "AAVE", "COMP", "MKR", "SUSHI", "CRV", "1INCH", "YFI"],
            "layer2_scaling": ["MATIC", "ARB", "OP", "IMX", "LRC", "MINA"],
            "smart_contracts": ["ETH", "ADA", "SOL", "AVAX", "DOT", "ALGO", "NEAR", "ATOM"],
            "meme_coins": ["DOGE", "SHIB", "PEPE", "FLOKI", "BONK"],
            "exchange_tokens": ["BNB", "CRO", "FTT", "HT", "KCS", "OKB"],
            "stablecoins": ["USDT", "USDC", "BUSD", "DAI", "FRAX", "TUSD"],
            "ai_gaming": ["FET", "AGIX", "OCEAN", "AXS", "SAND", "MANA", "ENJ"],
            "privacy_coins": ["XMR", "ZEC", "DASH", "SCRT"],
            "infrastructure": ["LINK", "GRT", "FIL", "AR", "STORJ", "SIA"]
        }
        
        diversity_stats = {}
        token_symbols = {token["symbol"] for token in tokens}
        
        for category, category_tokens in categories.items():
            found_tokens = [symbol for symbol in category_tokens if symbol in token_symbols]
            diversity_stats[category] = {
                "count": len(found_tokens),
                "tokens": found_tokens,
                "coverage": len(found_tokens) / len(category_tokens) * 100
            }
        
        # Statistiques globales
        total_categorized = sum(len(stats["tokens"]) for stats in diversity_stats.values())
        diversity_stats["global"] = {
            "total_tokens": len(tokens),
            "categorized_tokens": total_categorized,
            "uncategorized_tokens": len(tokens) - total_categorized,
            "diversity_score": len([cat for cat, stats in diversity_stats.items() 
                                  if cat != "global" and stats["count"] >= 3]) / len(categories) * 100
        }
        
        return diversity_stats
    
    def print_diversity_report(self, tokens: List[Dict]):
        """
        Affiche un rapport d√©taill√© de la diversit√© des tokens
        
        Args:
            tokens: Liste des tokens √† analyser
        """
        diversity_stats = self.analyze_token_diversity(tokens)
        
        print("\nüìä RAPPORT DE DIVERSIT√â DES TOKENS")
        print("=" * 50)
        
        # Statistiques globales
        global_stats = diversity_stats["global"]
        print(f"Total de tokens: {global_stats['total_tokens']}")
        print(f"Tokens cat√©goris√©s: {global_stats['categorized_tokens']}")
        print(f"Score de diversit√©: {global_stats['diversity_score']:.1f}%")
        print()
        
        # D√©tail par cat√©gorie
        print("Repr√©sentation par cat√©gorie:")
        print("-" * 30)
        
        for category, stats in diversity_stats.items():
            if category == "global":
                continue
                
            status = "‚úÖ" if stats["count"] >= 3 else ("‚ö†Ô∏è" if stats["count"] >= 1 else "‚ùå")
            category_name = category.replace("_", " ").title()
            
            print(f"{status} {category_name:<18} {stats['count']:2d}/10 ({stats['coverage']:4.1f}%)")
            if stats["tokens"]:
                tokens_str = ", ".join(stats["tokens"][:5])
                if len(stats["tokens"]) > 5:
                    tokens_str += f" (+{len(stats['tokens'])-5} autres)"
                print(f"    {tokens_str}")
        
        print()
    
    def get_top_100_tokens(self, save_to_file: bool = True) -> List[Dict]:
        """
        API principale : r√©cup√®re et fusionne les top 100 tokens
        
        Args:
            save_to_file: Sauvegarder le r√©sultat dans resultats_choix_des_100tokens.json
            
        Returns:
            Liste des 100 meilleurs tokens
        """
        logger.info("üöÄ R√©cup√©ration des top 100 tokens...")
        
        # R√©cup√©ration des donn√©es depuis les APIs
        marketcap_tokens = self.get_top_100_marketcap_coingecko()
        volume_tokens = self.get_top_100_volume_binance()
        
        if not marketcap_tokens and not volume_tokens:
            logger.error("‚ùå Impossible de r√©cup√©rer les donn√©es des APIs")
            return []
        
        # Fusion et s√©lection avec garantie de diversit√©
        top_100 = self.merge_and_select_top_100(marketcap_tokens, volume_tokens)
        
        # Analyse de la diversit√© finale
        diversity_stats = self.analyze_token_diversity(top_100)
        logger.info(f"üìä Analyse de diversit√©:")
        logger.info(f"   Score de diversit√©: {diversity_stats['global']['diversity_score']:.1f}%")
        logger.info(f"   Tokens cat√©goris√©s: {diversity_stats['global']['categorized_tokens']}/100")
        
        # Afficher les cat√©gories bien repr√©sent√©es
        well_represented = [cat for cat, stats in diversity_stats.items() 
                          if cat != "global" and stats["count"] >= 3]
        logger.info(f"   Cat√©gories bien repr√©sent√©es (‚â•3): {len(well_represented)}/10")
        
        # Sauvegarde optionnelle
        if save_to_file and top_100:
            try:
                with open(self.paths.tokens_json, 'w', encoding='utf-8') as f:
                    json.dump(top_100, f, indent=2, ensure_ascii=False)
                logger.info(f"‚úÖ Top 100 sauvegard√©: {self.paths.tokens_json}")
            except Exception as e:
                logger.error(f"Erreur sauvegarde: {e}")
        
        return top_100
    
    def load_saved_tokens(self) -> List[Dict]:
        """
        Charge les tokens sauvegard√©s depuis le fichier JSON
        
        Returns:
            Liste des tokens ou liste vide si erreur
        """
        try:
            if self.paths.tokens_json.exists():
                with open(self.paths.tokens_json, 'r', encoding='utf-8') as f:
                    tokens = json.load(f)
                logger.info(f"‚úÖ {len(tokens)} tokens charg√©s depuis {self.paths.tokens_json}")
                return tokens
            else:
                logger.warning(f"Fichier tokens non trouv√©: {self.paths.tokens_json}")
                return []
        except Exception as e:
            logger.error(f"Erreur chargement tokens: {e}")
            return []
    
    # =========================================================
    #  SECTION 2: T√©l√©chargement des donn√©es crypto
    # =========================================================
    
    def _interval_to_ms(self, interval: str) -> int:
        """Convertit un interval en millisecondes"""
        multipliers = {
            "m": 60 * 1000,
            "h": 60 * 60 * 1000,
            "d": 24 * 60 * 60 * 1000,
            "w": 7 * 24 * 60 * 60 * 1000
        }
        
        if interval[-1] in multipliers:
            return int(interval[:-1]) * multipliers[interval[-1]]
        return 60 * 1000  # Default 1 minute
    
    def _download_single_pair(self, symbol: str, interval: str, 
                             progress_callback: Optional[Callable] = None) -> bool:
        """
        T√©l√©charge les donn√©es pour une paire symbol/interval via Binance API
        
        Args:
            symbol: Symbol (ex: BTCUSDC)
            interval: Interval (ex: 1h)
            progress_callback: Callback optionnel pour progression
            
        Returns:
            True si succ√®s, False sinon
        """
        url = "https://api.binance.com/api/v3/klines"
        
        # Calcul p√©riode de t√©l√©chargement
        end_time = int(time.time() * 1000)
        start_time = end_time - (self.history_days * 24 * 60 * 60 * 1000)
        
        params = {
            "symbol": symbol,
            "interval": interval,
            "startTime": start_time,
            "endTime": end_time,
            "limit": self.binance_limit
        }
        
        try:
            logger.debug(f"T√©l√©chargement {symbol}_{interval}...")
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            
            if not data:
                logger.warning(f"Aucune donn√©e pour {symbol}_{interval}")
                return False
            
            # Conversion en DataFrame
            df = pd.DataFrame(data, columns=[
                'timestamp', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_volume', 'count', 'taker_buy_volume',
                'taker_buy_quote_volume', 'ignore'
            ])
            
            # Nettoyage et typage
            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
            
            for col in ['open', 'high', 'low', 'close', 'volume']:
                df[col] = pd.to_numeric(df[col])
            
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
            df = df.set_index('timestamp').sort_index()
            
            # Suppression des doublons et NaN
            df = df[~df.index.duplicated(keep='last')]
            df = df.dropna()
            
            if len(df) == 0:
                logger.warning(f"DataFrame vide apr√®s nettoyage: {symbol}_{interval}")
                return False
            
            # Sauvegarde JSON
            json_file = self.paths.json_root / f"{symbol}_{interval}.json"
            
            # Conversion pour JSON (timestamp en ms)
            df_json = df.reset_index()
            df_json['timestamp'] = df_json['timestamp'].astype(int) // 1_000_000
            
            with open(json_file, 'w') as f:
                json.dump(df_json.to_dict('records'), f)
            
            # Sauvegarde Parquet (plus efficace)
            parquet_file = self.paths.parquet_root / f"{symbol}_{interval}.parquet"
            df.to_parquet(parquet_file, compression='zstd')
            
            logger.debug(f"‚úÖ {symbol}_{interval}: {len(df)} lignes sauvegard√©es")
            
            if progress_callback:
                progress_callback(symbol, interval, len(df))
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur t√©l√©chargement {symbol}_{interval}: {e}")
            return False
    
    def download_crypto_data(self, symbols: List[str], intervals: Optional[List[str]] = None,
                           progress_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """
        T√©l√©charge les donn√©es crypto pour plusieurs symboles/intervals
        
        Args:
            symbols: Liste des symboles (ex: ["BTCUSDC", "ETHUSDC"])
            intervals: Liste des intervals (par d√©faut: ["3m", "5m", "15m", "30m", "1h"])
            progress_callback: Callback optionnel(symbol, interval, nb_rows)
            
        Returns:
            Dictionnaire avec statistiques de t√©l√©chargement
        """
        if intervals is None:
            intervals = self.intervals
        
        logger.info(f"üîÑ T√©l√©chargement de {len(symbols)} symboles √ó {len(intervals)} intervals...")
        
        # Pr√©paration des t√¢ches
        tasks = []
        for symbol in symbols:
            for interval in intervals:
                tasks.append((symbol, interval))
        
        # T√©l√©chargement parall√®le
        results = {"success": 0, "errors": 0, "details": []}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_task = {
                executor.submit(self._download_single_pair, symbol, interval, progress_callback): (symbol, interval)
                for symbol, interval in tasks
            }
            
            for future in as_completed(future_to_task):
                symbol, interval = future_to_task[future]
                try:
                    success = future.result()
                    if success:
                        results["success"] += 1
                        results["details"].append(f"‚úÖ {symbol}_{interval}")
                    else:
                        results["errors"] += 1
                        results["details"].append(f"‚ùå {symbol}_{interval}")
                except Exception as e:
                    results["errors"] += 1
                    results["details"].append(f"‚ùå {symbol}_{interval}: {e}")
        
        logger.info(f"‚úÖ T√©l√©chargement termin√©: {results['success']} succ√®s, {results['errors']} erreurs")
        return results
    
    def download_top_100_data(self, intervals: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        T√©l√©charge les donn√©es pour tous les top 100 tokens
        
        Args:
            intervals: Liste des intervals (optionnel)
            
        Returns:
            Statistiques de t√©l√©chargement
        """
        # Chargement des tokens
        tokens = self.load_saved_tokens()
        if not tokens:
            logger.info("Aucun token sauvegard√©, r√©cup√©ration des top 100...")
            tokens = self.get_top_100_tokens()
        
        if not tokens:
            logger.error("‚ùå Impossible de r√©cup√©rer les tokens")
            return {"success": 0, "errors": 1, "details": ["Pas de tokens disponibles"]}
        
        # Conversion en symboles USDC
        symbols = [token["symbol"] + "USDC" for token in tokens]
        
        logger.info(f"üöÄ T√©l√©chargement des donn√©es pour {len(symbols)} tokens...")
        return self.download_crypto_data(symbols, intervals)
    
    # =========================================================
    #  SECTION 3: Chargement et traitement des donn√©es
    # =========================================================
    
    def load_ohlcv_data(self, symbol: str, interval: str) -> Optional[pd.DataFrame]:
        """
        Charge les donn√©es OHLCV avec priorit√© Parquet ‚Üí JSON
        
        Args:
            symbol: Symbole (ex: BTCUSDC)
            interval: Interval (ex: 1h)
            
        Returns:
            DataFrame OHLCV avec DatetimeIndex UTC ou None
        """
        # Priorit√© 1: Parquet
        parquet_file = self.paths.parquet_root / f"{symbol}_{interval}.parquet"
        if parquet_file.exists():
            try:
                df = pd.read_parquet(parquet_file)
                logger.debug(f"Charg√© depuis Parquet: {symbol}_{interval} ({len(df)} lignes)")
                return df
            except Exception as e:
                logger.warning(f"Erreur lecture Parquet {parquet_file}: {e}")
        
        # Priorit√© 2: JSON
        json_file = self.paths.json_root / f"{symbol}_{interval}.json"
        if json_file.exists():
            try:
                with open(json_file, 'r') as f:
                    data = json.load(f)
                
                df = pd.DataFrame(data)
                
                # V√©rification des colonnes
                required_cols = ["timestamp", "open", "high", "low", "close", "volume"]
                if not all(col in df.columns for col in required_cols):
                    logger.error(f"Colonnes manquantes dans {json_file}")
                    return None
                
                # Conversion types
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
                df = df.set_index('timestamp').sort_index()
                
                for col in ["open", "high", "low", "close", "volume"]:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                
                df = df.dropna()
                
                # Cr√©ation automatique du Parquet pour optimiser les futurs acc√®s
                try:
                    df.to_parquet(parquet_file, compression="zstd")
                    logger.debug(f"Parquet cr√©√©: {parquet_file}")
                except Exception as e:
                    logger.warning(f"Impossible de cr√©er {parquet_file}: {e}")
                
                logger.debug(f"Charg√© depuis JSON: {symbol}_{interval} ({len(df)} lignes)")
                return df
                
            except Exception as e:
                logger.error(f"Erreur lecture JSON {json_file}: {e}")
        
        logger.warning(f"Aucune donn√©e trouv√©e pour {symbol}_{interval}")
        return None
    
    # =========================================================
    #  SECTION 4: Calcul des indicateurs techniques
    # =========================================================
    
    def calculate_rsi(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calcule le RSI"""
        delta = df['close'].diff()
        gain = delta.clip(lower=0).rolling(window=period, min_periods=period).mean()
        loss = (-delta.clip(upper=0)).rolling(window=period, min_periods=period).mean()
        rs = gain / loss.replace(0, np.nan)
        return 100 - (100 / (1 + rs))
    
    def calculate_bollinger_bands(self, df: pd.DataFrame, period: int = 20, std_dev: float = 2.0) -> Dict[str, pd.Series]:
        """Calcule les bandes de Bollinger"""
        close = df['close']
        ma = close.rolling(window=period, min_periods=period).mean()
        std = close.rolling(window=period, min_periods=period).std()
        
        return {
            'bb_middle': ma,
            'bb_upper': ma + (std_dev * std),
            'bb_lower': ma - (std_dev * std),
            'bb_width': (2 * std_dev * std) / ma
        }
    
    def calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calcule l'ATR (Average True Range)"""
        high_low = df['high'] - df['low']
        high_close = (df['high'] - df['close'].shift()).abs()
        low_close = (df['low'] - df['close'].shift()).abs()
        
        true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        return true_range.rolling(window=period, min_periods=period).mean()
    
    def calculate_ema(self, df: pd.DataFrame, period: int = 20) -> pd.Series:
        """Calcule l'EMA (Exponential Moving Average)"""
        return df['close'].ewm(span=period, adjust=False).mean()
    
    def calculate_macd(self, df: pd.DataFrame, fast: int = 12, slow: int = 26, signal: int = 9) -> Dict[str, pd.Series]:
        """Calcule le MACD"""
        ema_fast = df['close'].ewm(span=fast, adjust=False).mean()
        ema_slow = df['close'].ewm(span=slow, adjust=False).mean()
        macd_line = ema_fast - ema_slow
        signal_line = macd_line.ewm(span=signal, adjust=False).mean()
        histogram = macd_line - signal_line
        
        return {
            'macd': macd_line,
            'macd_signal': signal_line,
            'macd_histogram': histogram
        }
    
    def add_indicators(self, df: pd.DataFrame, indicators: List[str]) -> pd.DataFrame:
        """
        Ajoute plusieurs indicateurs √† un DataFrame OHLCV
        
        Args:
            df: DataFrame OHLCV
            indicators: Liste des indicateurs ('rsi', 'bollinger', 'atr', 'ema', 'macd')
            
        Returns:
            DataFrame avec indicateurs ajout√©s
        """
        result = df.copy()
        
        for indicator in indicators:
            try:
                if indicator == 'rsi':
                    result['rsi'] = self.calculate_rsi(df)
                elif indicator == 'bollinger':
                    bb_data = self.calculate_bollinger_bands(df)
                    for key, series in bb_data.items():
                        result[key] = series
                elif indicator == 'atr':
                    result['atr'] = self.calculate_atr(df)
                elif indicator == 'ema':
                    result['ema'] = self.calculate_ema(df)
                elif indicator == 'macd':
                    macd_data = self.calculate_macd(df)
                    for key, series in macd_data.items():
                        result[key] = series
                else:
                    logger.warning(f"Indicateur non support√©: {indicator}")
                    
            except Exception as e:
                logger.error(f"Erreur calcul {indicator}: {e}")
        
        # Suppression des lignes avec NaN
        result = result.dropna()
        
        return result
    
    # =========================================================
    #  SECTION 5: API principale unifi√©e
    # =========================================================
    
    def get_trading_data(self, symbol: str, interval: str, 
                        indicators: Optional[List[str]] = None,
                        start_date: Optional[str] = None,
                        end_date: Optional[str] = None) -> Optional[pd.DataFrame]:
        """
        API principale : charge les donn√©es OHLCV + indicateurs
        
        Args:
            symbol: Symbole crypto (ex: BTCUSDC)
            interval: Interval (ex: 1h, 5m)
            indicators: Liste des indicateurs √† calculer (optionnel)
            start_date: Date de d√©but (format YYYY-MM-DD, optionnel)
            end_date: Date de fin (format YYYY-MM-DD, optionnel)
            
        Returns:
            DataFrame complet avec OHLCV + indicateurs ou None
            
        Example:
            >>> manager = TradXProManager()
            >>> df = manager.get_trading_data("BTCUSDC", "1h", 
            ...                              indicators=["rsi", "bollinger", "atr"])
            >>> print(f"DataFrame: {len(df)} lignes, {len(df.columns)} colonnes")
        """
        # Chargement des donn√©es de base
        df = self.load_ohlcv_data(symbol, interval)
        
        if df is None:
            logger.error(f"Impossible de charger {symbol}_{interval}")
            return None
        
        # Filtrage temporel si demand√©
        if start_date or end_date:
            df = df.loc[start_date:end_date]
            logger.info(f"Filtrage temporel: {len(df)} lignes apr√®s filtrage")
        
        # Ajout des indicateurs si demand√©s
        if indicators:
            df = self.add_indicators(df, indicators)
            logger.info(f"Indicateurs ajout√©s: {indicators}")
        
        logger.info(f"‚úÖ {symbol}_{interval}: {len(df)} lignes, {len(df.columns)} colonnes")
        return df
    
    def get_multiple_trading_data(self, pairs: List[Tuple[str, str]], 
                                 indicators: Optional[List[str]] = None) -> Dict[str, pd.DataFrame]:
        """
        Charge les donn√©es pour plusieurs paires en parall√®le
        
        Args:
            pairs: Liste de tuples (symbol, interval)
            indicators: Liste des indicateurs √† calculer
            
        Returns:
            Dictionnaire {symbol_interval: DataFrame}
        """
        logger.info(f"Chargement de {len(pairs)} paires en parall√®le...")
        
        results = {}
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_pair = {
                executor.submit(self.get_trading_data, symbol, interval, indicators): (symbol, interval)
                for symbol, interval in pairs
            }
            
            for future in as_completed(future_to_pair):
                symbol, interval = future_to_pair[future]
                try:
                    df = future.result()
                    if df is not None:
                        results[f"{symbol}_{interval}"] = df
                        logger.debug(f"‚úÖ {symbol}_{interval} charg√©")
                    else:
                        logger.warning(f"‚ùå √âchec chargement {symbol}_{interval}")
                except Exception as e:
                    logger.error(f"Erreur {symbol}_{interval}: {e}")
        
        logger.info(f"‚úÖ {len(results)}/{len(pairs)} paires charg√©es avec succ√®s")
        return results
    
    # =========================================================
    #  SECTION 6: Utilitaires et statistiques
    # =========================================================
    
    def get_available_data(self) -> Dict[str, List[str]]:
        """
        Scanne les donn√©es disponibles sur disque
        
        Returns:
            Dictionnaire {symbol: [list_of_intervals]}
        """
        available = {}
        
        # Scan des fichiers Parquet
        for file_path in self.paths.parquet_root.glob("*.parquet"):
            filename = file_path.stem
            if "_" in filename:
                symbol, interval = filename.rsplit("_", 1)
                if symbol not in available:
                    available[symbol] = []
                available[symbol].append(interval)
        
        # Compl√©ter avec les fichiers JSON s'ils ne sont pas en Parquet
        for file_path in self.paths.json_root.glob("*.json"):
            filename = file_path.stem
            if "_" in filename and not filename.startswith("resultats"):
                symbol, interval = filename.rsplit("_", 1)
                if symbol not in available:
                    available[symbol] = []
                if interval not in available[symbol]:
                    available[symbol].append(interval)
        
        # Tri des intervals
        for symbol in available:
            available[symbol] = sorted(available[symbol])
        
        return available
    
    def get_data_statistics(self) -> Dict[str, Any]:
        """
        Calcule des statistiques sur les donn√©es disponibles
        
        Returns:
            Dictionnaire avec statistiques
        """
        available_data = self.get_available_data()
        
        total_files = sum(len(intervals) for intervals in available_data.values())
        
        # Taille des dossiers
        json_size = sum(f.stat().st_size for f in self.paths.json_root.glob("*.json")) / 1024 / 1024
        parquet_size = sum(f.stat().st_size for f in self.paths.parquet_root.glob("*.parquet")) / 1024 / 1024
        
        stats = {
            "symbols_count": len(available_data),
            "total_files": total_files,
            "intervals": list(set(interval for intervals in available_data.values() for interval in intervals)),
            "json_size_mb": round(json_size, 1),
            "parquet_size_mb": round(parquet_size, 1),
            "total_size_mb": round(json_size + parquet_size, 1),
            "top_symbols": sorted(available_data.keys())[:10] if available_data else [],
            "sample_data": dict(list(available_data.items())[:5]) if available_data else {}
        }
        
        return stats
    
    def cleanup_old_files(self, days_old: int = 7) -> Dict[str, int]:
        """
        Nettoie les fichiers anciens
        
        Args:
            days_old: Supprimer les fichiers plus anciens que X jours
            
        Returns:
            Statistiques de nettoyage
        """
        cutoff_time = time.time() - (days_old * 24 * 60 * 60)
        stats = {"json_removed": 0, "parquet_removed": 0}
        
        # Nettoyage JSON
        for file_path in self.paths.json_root.glob("*.json"):
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                stats["json_removed"] += 1
        
        # Nettoyage Parquet
        for file_path in self.paths.parquet_root.glob("*.parquet"):
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                stats["parquet_removed"] += 1
        
        logger.info(f"Nettoyage termin√©: {stats['json_removed']} JSON, {stats['parquet_removed']} Parquet supprim√©s")
        return stats

# =========================================================
#  SECTION 7: Interface en ligne de commande
# =========================================================

def main():
    """Interface en ligne de commande pour tester le gestionnaire"""
    print("üöÄ TradXPro Core Manager - Test Interface")
    print("=" * 50)
    
    manager = TradXProManager()
    
    while True:
        print("\nOptions disponibles:")
        print("1. üìä R√©cup√©rer top 100 tokens")
        print("2. üì• T√©l√©charger donn√©es crypto")
        print("3. üìà Charger donn√©es avec indicateurs")
        print("4. üìã Statistiques des donn√©es")
        print("5. üßπ Nettoyer anciens fichiers")
        print("0. ‚ùå Quitter")
        
        choice = input("\nVotre choix: ").strip()
        
        try:
            if choice == "1":
                print("\nüîÑ R√©cup√©ration des top 100 tokens...")
                tokens = manager.get_top_100_tokens()
                print(f"‚úÖ {len(tokens)} tokens r√©cup√©r√©s")
                for i, token in enumerate(tokens[:10], 1):
                    print(f"  {i:2d}. {token['symbol']:10s} - {token['name'][:30]:<30s} (Score: {token['score']:.1f})")
                
            elif choice == "2":
                symbols = input("Symboles (s√©par√©s par des virgules, ex: BTCUSDC,ETHUSDC): ").strip()
                if symbols:
                    symbol_list = [s.strip().upper() for s in symbols.split(",")]
                    print(f"\nüîÑ T√©l√©chargement de {len(symbol_list)} symboles...")
                    results = manager.download_crypto_data(symbol_list)
                    print(f"‚úÖ R√©sultats: {results['success']} succ√®s, {results['errors']} erreurs")
                
            elif choice == "3":
                symbol = input("Symbole (ex: BTCUSDC): ").strip().upper()
                interval = input("Interval (ex: 1h): ").strip()
                indicators_str = input("Indicateurs (ex: rsi,bollinger,atr): ").strip()
                
                indicators = [i.strip() for i in indicators_str.split(",") if i.strip()] if indicators_str else None
                
                print(f"\nüîÑ Chargement {symbol}_{interval} avec indicateurs {indicators}...")
                df = manager.get_trading_data(symbol, interval, indicators)
                
                if df is not None:
                    print(f"‚úÖ DataFrame charg√©: {len(df)} lignes, {len(df.columns)} colonnes")
                    print(f"Colonnes: {list(df.columns)}")
                    print(f"P√©riode: {df.index[0]} √† {df.index[-1]}")
                    print("\nAper√ßu des derni√®res valeurs:")
                    print(df.tail(3))
                else:
                    print("‚ùå Impossible de charger les donn√©es")
                
            elif choice == "4":
                print("\nüìä Calcul des statistiques...")
                stats = manager.get_data_statistics()
                print(f"‚úÖ Statistiques des donn√©es:")
                print(f"  Symboles: {stats['symbols_count']}")
                print(f"  Fichiers total: {stats['total_files']}")
                print(f"  Intervals disponibles: {stats['intervals']}")
                print(f"  Taille JSON: {stats['json_size_mb']} MB")
                print(f"  Taille Parquet: {stats['parquet_size_mb']} MB")
                print(f"  Taille totale: {stats['total_size_mb']} MB")
                
            elif choice == "5":
                days = input("Supprimer fichiers plus anciens que X jours (d√©faut: 7): ").strip()
                days = int(days) if days.isdigit() else 7
                print(f"\nüßπ Nettoyage des fichiers > {days} jours...")
                stats = manager.cleanup_old_files(days)
                print(f"‚úÖ {stats['json_removed'] + stats['parquet_removed']} fichiers supprim√©s")
                
            elif choice == "0":
                print("üëã Au revoir!")
                break
                
            else:
                print("‚ùå Choix invalide")
                
        except KeyboardInterrupt:
            print("\nüëã Au revoir!")
            break
        except Exception as e:
            logger.error(f"Erreur: {e}")

if __name__ == "__main__":
    main()