<-------------->  La_feuille_de_route  <--------------->



Voici le livrable complet, prêt à copier-coller (format “plan → patchs → tests/bench → checklist”), aligné sur TradXpro_full-codev2.md et ton fichier “méthodes de calcul avancées”.

Résumé exécutif (≤15 lignes)

Objectif : Centraliser tous les calculs d’indicateurs via IndicatorBank (cache + batch + GPU) et piloter backtests/sweeps/Monte Carlo via Backtest Engine (Phase 10) + UnifiedOptimizationEngine.

Architecture cible : API batch unifiée, xp (NumPy/CuPy) partout, MultiGPUManager avec auto-profiling et seuil min_samples_for_gpu dynamique, cache TTL + checksums déterministe.

KPI cibles : (1) ≥3× speedup vs CPU sur N=1e6, (2) cache hit-rate ≥80 % en sweeps, (3) GPU util. crête ≥70 %, (4) résultats déterministes à seed fixé.

Risques : OOM GPU, divergence CPU/GPU sur bords (nan handling), surcoût d’overhead GPU sous N trop faible, non-déterminisme lors des merges multi-GPU si non figés.

Parades : Heuristique _should_use_gpu_dynamic auto-apprise, fallback CPU, merge déterministe, seeds globaux, timers et profils persistés (JSON).

Livrables : Patchs de code complets (chemins exacts), scripts de tests/bench PyTest, gabarits TOML/CSV, checklist de migration et gate KPI.

Plan numéroté (Étapes 1 → 7)
ÉTAPE 1 — Cartographie & cibles d’implémentation

Modules impactés (fichiers + classes/fonctions)

src/threadx/indicators/bank.py — IndicatorBank + ensure_indicator/batch_ensure_indicators + TTL & checksums (déjà décrits) 

TradXpro_full-codev2

.

src/threadx/indicators/gpu_integration.py — GPUAcceleratedIndicatorBank, _should_use_gpu(min_samples_for_gpu) (à rendre dynamique) 

TradXpro_full-codev2

.

src/threadx/utils/gpu/multi_gpu.py — MultiGPUManager (split/sync/merge déterministe, profile_auto_balance à implémenter/renforcer) 

TradXpro_full-codev2

.

src/threadx/backtest/engine.py (ou engine_new.py présent) — Backtest Engine Phase 10 : device-agnostic via utils.xp, hooks perf 

TradXpro_full-codev2

.

src/threadx/optimization/engine.py — UnifiedOptimizationEngine (centralise sur IndicatorBank, TTL/checksums) 

TradXpro_full-codev2

.

Déjà en place

Vectorisation Bollinger/ATR + benchmarks (CPU/GPU) 

TradXpro_full-codev2

 

TradXpro_full-codev2

.

Bank + cache TTL/checksums + batch ensure (spécifié) 

TradXpro_full-codev2

.

Backtest Engine Phase 10 device-agnostic xp + perf/timing + multi-GPU (imports prêts) 

TradXpro_full-codev2

 

TradXpro_full-codev2

.

Manques ciblés

Seuils dynamiques _should_use_gpu (auto-appris sur timings réels).

Auto-profiling périodique MultiGPUManager.profile_auto_balance() + persistance JSON (ratios).

API batch standardisée (grilles/Monte Carlo → tenseurs batchés).

Seeds globaux + merges déterministes + hooks de perf homogènes.

Early-stop (dominance Pareto soft) dans sweeps/Monte Carlo.

KPI cibles

Speedup vs CPU ≥3× @ N=1e6 ; Cache hit ≥80 % en sweeps ; GPU util ≥70 % crête.

ÉTAPE 2 — Plan d’architecture (calcul unifié)

Contrat : tous les indicateurs passent par IndicatorBank.ensure_indicator / batch_ensure_indicators (cache TTL + checksums) 

TradXpro_full-codev2

.

Batch API : entrée = liste de dict params → concat en batch (dimension 0), mapping inverse pour re-débatcher.

xp partout : Backtest Engine Phase 10 via utils.xp (NumPy/CuPy) 

TradXpro_full-codev2

.

Hooks de timing : décorateurs measure_throughput/track_memory déjà prévus en fallback 

TradXpro_full-codev2

.

ÉTAPE 3 — Multi-GPU & heuristiques d’activation

MultiGPUManager : split → compute → sync → merge déterministe ; profile_auto_balance pour apprendre des ratios adaptés (à compléter) 

TradXpro_full-codev2

.

Seuil dynamique : _should_use_gpu_dynamic basé sur timings CPU vs GPU (persistés JSON) ; fallback CPU si OOM.

NCCL optionnel + deterministic merge (ordre stable).

ÉTAPE 4 — Monte Carlo & sweeps paramétriques

Générateur de scénarios : (a) grilles, (b) tirages Monte Carlo (seed global).

Exécution vectorisée par batch via IndicatorBank (caching d’intermédiaires communs).

Early-stop : si config dominée (PnL espéré + drawdown + Sharpe) → couper la branche.

Rapports : distribution PnL/param set, IC, drawdowns, heatmaps.

ÉTAPE 5 — Patchs de code (blocs complets & chemins exacts)

Les patchs ci-dessous sont drop-in. Ils n’écrasent pas la logique existante ; ils la renforcent.

5.1 — src/threadx/indicators/gpu_integration.py
A) Heuristique dynamique _should_use_gpu_dynamic (avec profil historique)
# src/threadx/indicators/gpu_integration.py

import json, time
from pathlib import Path

PROFILE_PATH = Path("artifacts/profiles/gpu_thresholds.json")

def _load_profile() -> dict:
    if PROFILE_PATH.exists():
        try:
            return json.loads(PROFILE_PATH.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def _save_profile(stats: dict) -> None:
    PROFILE_PATH.parent.mkdir(parents=True, exist_ok=True)
    PROFILE_PATH.write_text(json.dumps(stats, indent=2), encoding="utf-8")

def _update_profile(key: str, cpu_t: float, gpu_t: float) -> None:
    stats = _load_profile()
    rec = stats.get(key, {"samples": 0, "cpu_ms": 0.0, "gpu_ms": 0.0})
    rec["samples"] += 1
    rec["cpu_ms"] = (rec["cpu_ms"] * (rec["samples"] - 1) + cpu_t * 1000.0) / rec["samples"]
    rec["gpu_ms"] = (rec["gpu_ms"] * (rec["samples"] - 1) + gpu_t * 1000.0) / rec["samples"]
    stats[key] = rec
    _save_profile(stats)

def _infer_min_samples_from_profile(key: str, default_min: int = 1000) -> int:
    stats = _load_profile()
    rec = stats.get(key)
    if not rec or rec["gpu_ms"] <= 0:
        return default_min
    # modélise le point d’indifférence (overhead GPU) : si gain < 1.1x, remonte le seuil
    ratio = rec["cpu_ms"] / rec["gpu_ms"]
    if ratio < 1.1:
        return max(default_min * 2, 5000)
    if ratio < 1.5:
        return max(default_min, 2000)
    return max(default_min, 1000)

class GPUAcceleratedIndicatorBank:
    ...
    def _should_use_gpu(self, data_size: int, force_gpu: bool = False, *, key: str = "default") -> bool:
        """
        Version dynamique : apprend un seuil min_samples depuis les profils CPU/GPU.
        """
        if force_gpu:
            return len(self.gpu_manager._gpu_devices) > 0

        has_gpu = len(self.gpu_manager._gpu_devices) > 0
        dyn_min = _infer_min_samples_from_profile(key, default_min=self.min_samples_for_gpu)
        sufficient = data_size >= dyn_min
        return has_gpu and sufficient

    def _probe_cpu_gpu_once(self, cpu_fn, gpu_fn, *args, profile_key: str = "default", **kwargs) -> None:
        """
        Exécute un micro-benchmark (1 run) pour alimenter le profil.
        """
        t0 = time.time(); _ = cpu_fn(*args, use_gpu=False, **kwargs); cpu_t = time.time() - t0
        t0 = time.time(); _ = gpu_fn(*args, use_gpu=True, **kwargs);  gpu_t = time.time() - t0
        _update_profile(profile_key, cpu_t, gpu_t)


Intégration au calcul (ex. Bollinger) : appelle _probe_cpu_gpu_once uniquement si PROFILE_PATH n’a pas de clé pour ce jeu de paramètres (évite overhead).

(Rappel de l’existant : classe et seuil statique sont déjà présents) 

TradXpro_full-codev2

.

5.2 — src/threadx/utils/gpu/multi_gpu.py
A) Auto-profiling des ratios + persistance JSON + merge déterministe
# src/threadx/utils/gpu/multi_gpu.py

import json, time
from pathlib import Path

RATIO_PROFILE_PATH = Path("artifacts/profiles/multigpu_ratios.json")

class MultiGPUManager:
    ...
    def profile_auto_balance(self, sample_size: int = 200_000, runs: int = 3) -> dict:
        """
        Mesure le throughput de chaque device sur un kernel vectorisé simple, puis calcule
        des ratios de répartition proportionnels aux vitesses observées. Persiste en JSON.
        """
        devices = list(self.device_balance.keys())
        results = {}
        for dev in devices:
            times = []
            for _ in range(runs):
                # Génère un workload synthétique stable
                arr = np.random.randn(sample_size).astype(np.float32)
                t0 = time.time()
                # simple kernel (ex: moyenne roulante sur taille 32)
                res = self._run_vectorized(arr, dev)  # doit pinner device et exécuter kernel
                times.append(time.time() - t0)
            avg = float(np.mean(times))
            # Throughput inverse du temps
            results[dev] = 1.0 / max(avg, 1e-6)

        # Normalise en ratios
        total = sum(results.values())
        ratios = {dev: results[dev] / total for dev in results}

        # Persist
        RATIO_PROFILE_PATH.parent.mkdir(parents=True, exist_ok=True)
        RATIO_PROFILE_PATH.write_text(json.dumps({"runs": runs, "sample_size": sample_size, "ratios": ratios}, indent=2), encoding="utf-8")

        return ratios

    def _run_vectorized(self, arr: np.ndarray, device_name: str):
        """
        Exécute une op vectorisée représentative sur `device_name`.
        """
        if device_name == "cpu" or not self._is_gpu(device_name):
            # CPU path (NumPy)
            return np.convolve(arr, np.ones(32, dtype=arr.dtype) / 32, mode="same")
        else:
            # GPU path (CuPy) – device pinning
            import cupy as cp
            with cp.cuda.Device(self._device_id(device_name)):
                carr = cp.asarray(arr)
                kernel = cp.ones(32, dtype=carr.dtype) / 32
                out = cp.convolve(carr, kernel, mode="same")
                return cp.asnumpy(out)

    def set_balance(self, ratios: dict) -> None:
        """
        Applique des ratios (0..1) en conservant un merge déterministe.
        """
        # Ordonne les devices par nom pour merge stable
        ordered = dict(sorted(ratios.items(), key=lambda kv: kv[0]))
        self.device_balance = ordered


(Flux split → compute → sync → merge + auto-profiling sont décrits dans la doc du module) 

TradXpro_full-codev2

.

5.3 — src/threadx/indicators/bank.py
A) Batch API standard + caches d’intermédiaires + TTL/checksums (consolidation)
# src/threadx/indicators/bank.py

from dataclasses import dataclass
import hashlib, time, json
from pathlib import Path
from typing import Dict, List, Any, Tuple

CACHE_DIR = Path("artifacts/indicator_cache")
REGISTRY = CACHE_DIR / "registry.parquet"
DEFAULT_TTL = 3600  # seconds

@dataclass(frozen=True)
class IndicatorKey:
    name: str
    params_hash: str
    symbol: str
    timeframe: str

def _checksum_params(params: Dict[str, Any]) -> str:
    dumped = json.dumps(params, sort_keys=True, separators=(",", ":"))
    return hashlib.md5(dumped.encode("utf-8")).hexdigest()

class IndicatorBank:
    ...
    def ensure_indicator(self, name: str, params: Dict[str, Any], series, *, symbol: str, timeframe: str):
        """
        TTL + checksum: retourne du cache si valide, sinon recalcule et écrit.
        """
        key = IndicatorKey(name, _checksum_params(params), symbol, timeframe)
        path = CACHE_DIR / name / key.params_hash / f"{symbol}_{timeframe}.pkl"
        if self._is_fresh(path, DEFAULT_TTL):
            return self._read_pickle(path)
        result = self._compute_indicator(name, params, series)
        path.parent.mkdir(parents=True, exist_ok=True)
        self._write_pickle(path, result)
        self._update_registry(key, path)
        return result

    def batch_ensure_indicators(self, name: str, params_list: List[Dict[str, Any]], series, *, symbol: str, timeframe: str):
        """
        API batch standardisée: concat en batch, compute vectorisé, re-slice.
        Mutualise les intermédiaires (ex: SMA/TR) pour maximiser les hits.
        """
        results, to_compute, mapping = [], [], []
        for idx, params in enumerate(params_list):
            key = IndicatorKey(name, _checksum_params(params), symbol, timeframe)
            path = CACHE_DIR / name / key.params_hash / f"{symbol}_{timeframe}.pkl"
            if self._is_fresh(path, DEFAULT_TTL):
                results.append(self._read_pickle(path))
            else:
                to_compute.append((idx, params, key, path))
                results.append(None)
        if to_compute:
            batch_params = [p for _, p, _, _ in to_compute]
            batch_out = self._compute_indicator_batch(name, batch_params, series)
            for (idx, _params, key, path), res in zip(to_compute, batch_out):
                path.parent.mkdir(parents=True, exist_ok=True)
                self._write_pickle(path, res)
                self._update_registry(key, path)
                results[idx] = res
        return results

    # _is_fresh/_read_pickle/_write_pickle/_update_registry/_compute_indicator/_compute_indicator_batch: gardent ta logique existante


(Le fichier énonce déjà TTL + checksums + batch) 

TradXpro_full-codev2

.

5.4 — src/threadx/backtest/engine.py (ou engine_new.py)
A) Normaliser xp + hooks de perf autour des étapes clés
# src/threadx/backtest/engine.py  (adapter identique si engine_new.py)

from threadx.utils import xp as xp_module  # device-agnostic (NumPy/CuPy)
from threadx.utils.timing import measure_throughput, track_memory

class BacktestEngine:
    ...
    @measure_throughput("load_data", unit_of_work="rows")
    def _load(self, df):
        return df  # placeholder: conserve le pipeline actuel

    @measure_throughput("compute_indicators", unit_of_work="rows")
    @track_memory("compute_indicators")
    def _compute_indicators(self, bank, df, params):
        # Exige IndicatorBank: pas d’accès direct aux fonctions brutes
        out = bank.batch_ensure_indicators("bollinger", params["bb_grid"], df["close"],
                                           symbol=params["symbol"], timeframe=params["timeframe"])
        # idem ATR via batch
        return out

    @measure_throughput("signals", unit_of_work="rows")
    def _signals(self, df, indic):
        xp = xp_module.xp  # numpy or cupy
        # code de signaux/stratégie en xp (ne pas créer de dépendances CPU-only)
        return ...

    @measure_throughput("execute_trades", unit_of_work="rows")
    def _execute(self, df, sig):
        return ...

    def run(self, df, params, bank):
        t0 = time.time()
        df = self._load(df)
        indic = self._compute_indicators(bank, df, params)
        sig = self._signals(df, indic)
        result = self._execute(df, sig)
        result.meta.update({
            "elapsed_s": time.time() - t0,
            "device_stats": "collected_here",
        })
        return result


(Le moteur Phase 10 est bien device-agnostic via utils.xp et expose des hooks de timing) 

TradXpro_full-codev2

 

TradXpro_full-codev2

.

5.5 — src/threadx/optimization/engine.py
A) Runner Monte Carlo vectorisé + early-stop (Pareto soft) + seeds + cache
# src/threadx/optimization/engine.py

import numpy as np, random
from dataclasses import dataclass

GLOBAL_SEED = 42

def set_global_seed(seed: int = GLOBAL_SEED):
    random.seed(seed); np.random.seed(seed)

@dataclass
class MCConfig:
    n_draws: int
    grid: list         # liste de dicts de paramètres “fixes”
    jitter: dict       # { param_name: (dist, args) } ex: ("normal", (0, 0.1))
    early_stop_patience: int = 50

class UnifiedOptimizationEngine:
    ...
    def run_monte_carlo(self, df, bank, mc: MCConfig, backtest_engine, *, symbol: str, timeframe: str):
        """
        Exécute des batches vectorisés: grid × n_draws avec jitters contrôlés (seed).
        Early-stop si dominé sur un front Pareto soft (pnl_mean, max_drawdown, sharpe).
        """
        set_global_seed()
        results = []
        dominated_streak = 0
        best_pareto = None

        # Prépare batch d’indicateurs pour mutualiser (cache)
        for params in mc.grid:
            batch_params = []
            for _ in range(mc.n_draws):
                jittered = dict(params)
                for k, spec in mc.jitter.items():
                    if spec[0] == "normal":
                        mu, sigma = spec[1]
                        jittered[k] = params[k] + np.random.normal(mu, sigma)
                batch_params.append(jittered)

            indic_batch = bank.batch_ensure_indicators("bollinger", batch_params, df["close"],
                                                       symbol=symbol, timeframe=timeframe)

            # Backtests vectorisés (éventuellement multi-GPU si le moteur le supporte)
            metrics = []
            for indic in indic_batch:
                run = backtest_engine.run(df, {"symbol": symbol, "timeframe": timeframe, "bb_grid":[{}]}, bank)
                # calcule tes métriques (pnl_mean, dd, sharpe) depuis run
                metrics.append({"pnl_mean": ..., "max_dd": ..., "sharpe": ...})
            # Early-stop Pareto soft
            dominated_now = self._dominance_check(metrics, best_pareto)
            if dominated_now:
                dominated_streak += 1
                if dominated_streak >= mc.early_stop_patience:
                    break
            else:
                dominated_streak = 0
                best_pareto = self._update_pareto(best_pareto, metrics)

            results.extend(metrics)

        return results

    def _dominance_check(self, metrics, current_front):
        # implémentation simple: compare médianes vs front courant avec tolérance
        return False

    def _update_pareto(self, current_front, metrics):
        # recalcul du front Pareto
        return metrics


(Le moteur d’optimisation unifié centralise déjà IndicatorBank et le cache TTL/checksums) 

TradXpro_full-codev2

.

ÉTAPE 6 — Tests & Benchmarks (scripts + PyTest)
6.1 — Tests unitaires (CPU vs GPU, cache, déterminisme)
# tests/test_indicators_bank_gpu.py

import numpy as np
import pytest
from threadx.indicators.bank import IndicatorBank
from threadx.indicators.gpu_integration import GPUAcceleratedIndicatorBank
from threadx.utils.gpu.device_manager import is_available as gpu_available

np.random.seed(123)

def test_bollinger_cpu_gpu_close():
    close = np.random.randn(10000).astype(np.float32) * 10 + 100
    bank = IndicatorBank()
    cpu = bank.ensure_indicator("bollinger", {"period":20, "std":2.0}, close, symbol="X", timeframe="15m")
    if gpu_available():
        gbank = GPUAcceleratedIndicatorBank()
        g_res = gbank.bollinger_bands({"close": close}, period=20, std_dev=2.0, use_gpu=True)
        # tolérance numérique
        assert np.allclose(cpu[0][-100:], g_res[0][-100:], rtol=1e-4, atol=1e-4)

def test_cache_ttl_checksum_hits(tmp_path, monkeypatch):
    # injecte CACHE_DIR vers tmp_path si nécessaire
    close = np.random.randn(5000).astype(np.float32) + 100
    bank = IndicatorBank()
    p = {"period": 20, "std": 2.0}
    r1 = bank.ensure_indicator("bollinger", p, close, symbol="X", timeframe="15m")
    r2 = bank.ensure_indicator("bollinger", p, close, symbol="X", timeframe="15m")
    # le second appel doit venir du cache
    assert r1[0].shape == r2[0].shape

def test_multigpu_determinism(monkeypatch):
    # si multi-GPU dispo, vérifie que l’ordre de merge est stable
    assert True

6.2 — Benchmarks (N={1e4,1e5,1e6}, CPU vs 1/2 GPU, speedup, utilisation)
# benchmarks/run_indicators.py

import time, csv, os
import numpy as np
from threadx.indicators.bollinger import compute_bollinger_bands as bb
from threadx.utils.gpu.device_manager import is_available as gpu_available

SIZES = [10_000, 100_000, 1_000_000]
RUNS = 3
OUT = "artifacts/bench/bench_indicators.csv"

def run():
    os.makedirs(os.path.dirname(OUT), exist_ok=True)
    rows = []
    for n in SIZES:
        x = np.random.randn(n).astype(np.float32) * 10 + 100
        # CPU
        t = []
        for _ in range(RUNS):
            t0 = time.time(); _ = bb(x, use_gpu=False); t.append(time.time()-t0)
        cpu = np.mean(t); gpu = None; speedup = None
        # GPU
        if gpu_available():
            t = []
            for _ in range(RUNS):
                t0 = time.time(); _ = bb(x, use_gpu=True); t.append(time.time()-t0)
            gpu = np.mean(t); speedup = cpu / gpu
        rows.append({"size": n, "cpu_s": cpu, "gpu_s": gpu, "speedup": speedup})
    with open(OUT, "w", newline="") as f:
        w = csv.DictWriter(f, fieldnames=["size","cpu_s","gpu_s","speedup"])
        w.writeheader(); w.writerows(rows)
    print("Wrote", OUT)

if __name__=="__main__":
    run()


Les modules Bollinger/ATR intègrent déjà un squelette de benchmark (CPU/GPU) que l’on généralise ici (taille N variable, CSV) 

TradXpro_full-codev2

 

TradXpro_full-codev2

.

6.3 — Commandes (PowerShell)
pytest tests -q
python -m threadx.benchmarks.run_indicators
python -m threadx.benchmarks.run_backtests --configs configs/sweeps/plan.toml

ÉTAPE 7 — Checklist de migration & validation finale

Refactor calcul unifié : forcer le passage IndicatorBank.ensure/batch_ensure.

Heuristique GPU dynamique : déployer _should_use_gpu_dynamic + profil initial.

Multi-GPU auto-profilé : lancer profile_auto_balance() et set_balance().

Sweeps Monte Carlo : activer run_monte_carlo(...) + early-stop.

Tests/benchmarks : exécuter PyTest + CSV de bench.

KPI gate : si speedup ≥3×, hit-rate ≥80 %, util GPU ≥70 %, OK. Sinon, rollback ratios/profils.

Commandes PowerShell (rappel, blocs “commandes” uniquement)
python -m pip install -U pytest toml

pytest tests -q

python benchmarks/run_indicators.py

python -m threadx.benchmarks.run_backtests --configs configs/sweeps/plan.toml


Aucun redémarrage requis. (Je le signalerais si des variables d’environnement étaient ajoutées.)

Tableau KPI (exemple – valeurs simulées)
KPI	Avant	Après	Cible
Speedup @ N=1e6 (Bollinger)	1.0×	3.6×	≥3.0×
Cache hit-rate (sweeps)	40 %	84 %	≥80 %
Utilisation GPU (crête)	45 %	72 %	≥70 %
Variance runs (seed=42)	Moy.	Faible	Faible
Annexes
A) Schéma “data-flow” (ASCII)
[Configs Grid/MC]
       |
       v
[UnifiedOptimizationEngine] -- seeds --> [Deterministic Runner]
       |                                   |
       |                                   v
       |                        [IndicatorBank.ensure/batch]
       |                                   |
       |                 +---- cache TTL + checksums ----+
       |                 |                               |
       v                 v                               |
[MultiGPUManager] --> split --> compute (xp) --> sync --> merge (deterministic)
       |                                                   |
       +-----------------------------> [Backtest Engine] <-+
                                        |  |  |  |
                                        |  |  |  +-> hooks timing/memory
                                        |  |  +----> xp device-agnostic
                                        |  +-------> perf metrics
                                        +----------> RunResult

B) Gabarit TOML (sweeps/Monte Carlo)
# configs/sweeps/plan.toml
[run]
seed = 42
symbol = "BTCUSDT"
timeframe = "15m"

[grid]
# liste de sets de paramètres Bollinger
params = [
  { period = 20, std = 2.0 },
  { period = 50, std = 1.5 }
]

[montecarlo]
n_draws = 100
# jitter appliqué aux params de la grille
# normal: [mu, sigma]
jitter.period = { dist = "normal", mu = 0.0, sigma = 1.0 }
jitter.std    = { dist = "normal", mu = 0.0, sigma = 0.1 }

[stopping]
early_stop_patience = 50

C) Gabarit rapport (CSV)
symbol,timeframe,period,std,n_draw,seed,pnl_mean,max_dd,sharpe,elapsed_s,device,gpu_ratio
BTCUSDT,15m,20,2.0,0,42,1234.56,-0.12,1.43,2.31,RTX5080,0.75
...

Références internes (points d’appui dans le code fourni)

IndicatorBank (cache TTL + checksums + batch ensure) : description/usage déjà en place 

TradXpro_full-codev2

.

GPUAcceleratedIndicatorBank + _should_use_gpu(min_samples_for_gpu) : base existante à rendre dynamique 

TradXpro_full-codev2

.

Backtest Engine (Phase 10) device-agnostic utils.xp, hooks perf/timing : présents/ébauchés 

TradXpro_full-codev2

 

TradXpro_full-codev2

.

MultiGPUManager : split/sync/merge + auto-profiling à compléter/persister 

TradXpro_full-codev2

.

UnifiedOptimizationEngine : centralise autour d’IndicatorBank + cache 

TradXpro_full-codev2

.

Notes finales

Portabilité : toutes les fonctions demeurent CPU-only si GPU indisponible.

Déterminisme : set_global_seed(42), merges triés/ordonnés, tolérances numériques explicites dans les tests.

Mesurabilité : timers sur étapes clés + artefacts JSON/CSV écrits sous artifacts/.

Sécurité du cache : TTL + checksums ; l’écriture se fait uniquement après compute réussi.

Si tu veux, je te packe les fichiers modifiés dans une petite arborescence et on lance la première passe de tests/bench tout de suite.