# üìö GUIDE DES BONNES PRATIQUES - ThreadX Data Management

## Date: 11 octobre 2025
## Objectif: Utilisation quotidienne optimale du syst√®me consolid√©

---

## üéØ WORKFLOW RECOMMAND√â

### Sc√©nario 1: Mise √† Jour Quotidienne des Tokens

**Fr√©quence:** 1x par jour (matin, avant trading)

**Script recommand√©:** `update_daily_tokens.py`

```python
#!/usr/bin/env python3
"""
Mise √† jour quotidienne des top tokens et leurs donn√©es OHLCV.
√Ä ex√©cuter chaque matin avant le d√©but du trading.
"""

import sys
from pathlib import Path
from datetime import datetime

# Import modules consolid√©s
sys.path.insert(0, str(Path(__file__).parent))

import importlib.util

# Import direct TokenManager (√©vite d√©pendances config)
spec = importlib.util.spec_from_file_location(
    "tokens",
    Path(__file__).parent / "src" / "threadx" / "data" / "tokens.py"
)
tokens_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(tokens_module)
TokenManager = tokens_module.TokenManager

# Import direct BinanceDataLoader
spec = importlib.util.spec_from_file_location(
    "loader",
    Path(__file__).parent / "src" / "threadx" / "data" / "loader.py"
)
loader_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(loader_module)
BinanceDataLoader = loader_module.BinanceDataLoader


def update_daily_tokens():
    """Mise √† jour quotidienne compl√®te des tokens."""
    
    print("=" * 60)
    print(f"üìÖ MISE √Ä JOUR QUOTIDIENNE - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print("=" * 60)
    
    # √âTAPE 1: Mise √† jour liste tokens top 100
    print("\nüìù √âTAPE 1/3: R√©cup√©ration top 100 tokens...")
    print("-" * 60)
    
    token_mgr = TokenManager(
        cache_path=Path("data/crypto_data_json/tokens_top100.json")
    )
    
    # R√©cup√©rer top 100 avec donn√©es fra√Æches
    top_tokens = token_mgr.get_top_tokens(
        limit=100,
        usdc_only=True,
        force_refresh=True  # Force rafra√Æchissement quotidien
    )
    
    print(f"‚úÖ {len(top_tokens)} tokens USDC s√©lectionn√©s")
    print(f"   Top 5: {top_tokens[:5]}")
    
    # √âTAPE 2: T√©l√©chargement donn√©es OHLCV
    print("\n\nüì• √âTAPE 2/3: T√©l√©chargement OHLCV...")
    print("-" * 60)
    
    loader = BinanceDataLoader(
        json_cache_dir=Path("data/crypto_data_json"),
        parquet_cache_dir=Path("data/crypto_data_parquet")
    )
    
    # Timeframes √† mettre √† jour (selon strat√©gie)
    timeframes = ["1h", "4h"]  # Adapter selon besoins
    
    for tf in timeframes:
        print(f"\n‚è±Ô∏è  Timeframe: {tf}")
        
        # T√©l√©chargement avec callback progression
        def progress_callback(pct, done, total):
            if done % 10 == 0:  # Afficher tous les 10 symboles
                print(f"   [{done}/{total}] {pct:.1f}%")
        
        results = loader.download_multiple(
            symbols=top_tokens,
            interval=tf,
            days_history=365,  # 1 an d'historique
            max_workers=4,     # Parall√®le pour vitesse
            progress_callback=progress_callback
        )
        
        print(f"   ‚úÖ {len(results)}/{len(top_tokens)} symboles t√©l√©charg√©s ({tf})")
    
    # √âTAPE 3: R√©sum√©
    print("\n\nüìä √âTAPE 3/3: R√©sum√©")
    print("-" * 60)
    
    # Statistiques stockage
    json_dir = Path("data/crypto_data_json")
    parquet_dir = Path("data/crypto_data_parquet")
    
    json_files = len(list(json_dir.glob("*.json")))
    parquet_files = len(list(parquet_dir.glob("*.parquet")))
    
    print(f"üìÅ Fichiers JSON:    {json_files}")
    print(f"üìÅ Fichiers Parquet: {parquet_files}")
    print(f"‚úÖ Mise √† jour termin√©e!")
    
    return {
        "tokens_count": len(top_tokens),
        "timeframes": timeframes,
        "json_files": json_files,
        "parquet_files": parquet_files
    }


if __name__ == "__main__":
    stats = update_daily_tokens()
    print("\n" + "=" * 60)
    print("‚úÖ MISE √Ä JOUR QUOTIDIENNE COMPL√àTE")
    print("=" * 60)
```

---

### Sc√©nario 2: Analyse d'un Token Sp√©cifique

**Fr√©quence:** √Ä la demande (analyse technique)

**Script recommand√©:** `analyze_token.py`

```python
#!/usr/bin/env python3
"""
Analyse technique compl√®te d'un token sp√©cifique.
T√©l√©charge donn√©es + calcule tous les indicateurs.
"""

import sys
from pathlib import Path
import pandas as pd

sys.path.insert(0, str(Path(__file__).parent))

import importlib.util

# Import modules
spec = importlib.util.spec_from_file_location(
    "loader",
    Path(__file__).parent / "src" / "threadx" / "data" / "loader.py"
)
loader_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(loader_module)
BinanceDataLoader = loader_module.BinanceDataLoader

spec = importlib.util.spec_from_file_location(
    "indicators_np",
    Path(__file__).parent / "src" / "threadx" / "indicators" / "indicators_np.py"
)
indicators_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(indicators_module)


def analyze_token(symbol: str, timeframe: str = "1h", days: int = 30):
    """
    Analyse technique compl√®te d'un token.
    
    Args:
        symbol: Symbole (ex: "BTCUSDC")
        timeframe: Timeframe (ex: "1h", "4h")
        days: Nombre de jours historique
        
    Returns:
        DataFrame avec OHLCV + tous indicateurs
    """
    
    print(f"üîç ANALYSE TECHNIQUE: {symbol} ({timeframe})")
    print("-" * 60)
    
    # 1. T√©l√©chargement donn√©es
    print(f"üì• T√©l√©chargement {days} jours de donn√©es...")
    
    loader = BinanceDataLoader(
        json_cache_dir=Path("data/crypto_data_json"),
        parquet_cache_dir=Path("data/crypto_data_parquet")
    )
    
    df = loader.download_ohlcv(
        symbol=symbol,
        interval=timeframe,
        days_history=days,
        force_update=False  # Utiliser cache si disponible
    )
    
    if df.empty:
        print(f"‚ùå Impossible de t√©l√©charger {symbol}")
        return None
    
    print(f"‚úÖ {len(df)} bougies charg√©es")
    
    # 2. Calcul indicateurs
    print(f"\nüìä Calcul indicateurs...")
    
    close = df['close'].values
    high = df['high'].values
    low = df['low'].values
    volume = df['volume'].values
    
    # RSI
    df['rsi'] = indicators_module.rsi_np(close, period=14)
    
    # EMA rapide et lente
    df['ema_12'] = indicators_module.ema_np(close, span=12)
    df['ema_26'] = indicators_module.ema_np(close, span=26)
    
    # Bollinger Bands
    lower, ma, upper, z = indicators_module.boll_np(close, period=20, std=2.0)
    df['bb_lower'] = lower
    df['bb_middle'] = ma
    df['bb_upper'] = upper
    df['bb_zscore'] = z
    
    # MACD
    macd, signal, hist = indicators_module.macd_np(close, fast=12, slow=26, signal=9)
    df['macd'] = macd
    df['macd_signal'] = signal
    df['macd_hist'] = hist
    
    # ATR
    df['atr'] = indicators_module.atr_np(high, low, close, period=14)
    
    # VWAP
    df['vwap'] = indicators_module.vwap_np(close, high, low, volume, window=96)
    
    # OBV
    df['obv'] = indicators_module.obv_np(close, volume)
    
    # Vortex
    vortex_df = indicators_module.vortex_df(high, low, close, period=14)
    df['vi_plus'] = vortex_df['vi_plus']
    df['vi_minus'] = vortex_df['vi_minus']
    
    print(f"‚úÖ {len(df.columns) - 5} indicateurs calcul√©s")
    
    # 3. Sauvegarde r√©sultats
    output_path = Path(f"data/exports/{symbol}_{timeframe}_analysis.parquet")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(output_path, engine="pyarrow", compression="zstd")
    
    print(f"üíæ R√©sultats sauvegard√©s: {output_path}")
    
    # 4. R√©sum√© statistiques
    print(f"\nüìà R√âSUM√â STATISTIQUES:")
    print(f"   Prix actuel:   ${df['close'].iloc[-1]:.4f}")
    print(f"   RSI:           {df['rsi'].iloc[-1]:.2f}")
    print(f"   MACD:          {df['macd'].iloc[-1]:.4f}")
    print(f"   ATR:           {df['atr'].iloc[-1]:.4f}")
    print(f"   BB Position:   {df['bb_zscore'].iloc[-1]:.2f} œÉ")
    
    return df


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Analyse technique token")
    parser.add_argument("symbol", help="Symbole (ex: BTCUSDC)")
    parser.add_argument("--timeframe", default="1h", help="Timeframe (d√©faut: 1h)")
    parser.add_argument("--days", type=int, default=30, help="Jours historique (d√©faut: 30)")
    
    args = parser.parse_args()
    
    df = analyze_token(args.symbol, args.timeframe, args.days)
    
    if df is not None:
        print("\n" + "=" * 60)
        print("‚úÖ ANALYSE TERMIN√âE")
        print("=" * 60)
```

**Utilisation:**
```bash
# Analyser BTCUSDC sur 30 jours
python analyze_token.py BTCUSDC

# Analyser ETHUSDC en 4h sur 90 jours
python analyze_token.py ETHUSDC --timeframe 4h --days 90
```

---

### Sc√©nario 3: Scan Complet Multi-Tokens

**Fr√©quence:** 2-3x par jour (recherche opportunit√©s)

**Script recommand√©:** `scan_all_tokens.py`

```python
#!/usr/bin/env python3
"""
Scan complet de tous les top tokens avec crit√®res de filtrage.
Trouve les meilleures opportunit√©s selon indicateurs.
"""

import sys
from pathlib import Path
import pandas as pd
from typing import List, Dict

sys.path.insert(0, str(Path(__file__).parent))

import importlib.util

# Imports modules (comme pr√©c√©demment)
# ... code import ...


def scan_all_tokens(
    tokens: List[str],
    timeframe: str = "1h",
    filters: Dict = None
):
    """
    Scan complet avec filtres.
    
    Args:
        tokens: Liste symboles √† scanner
        timeframe: Timeframe analyse
        filters: Crit√®res de filtrage (RSI, MACD, etc.)
        
    Returns:
        DataFrame r√©sultats filtr√©s tri√©s par score
    """
    
    if filters is None:
        filters = {
            "rsi_min": 30,
            "rsi_max": 70,
            "macd_positive": True,
            "bb_zscore_min": -2.0,
            "bb_zscore_max": 2.0
        }
    
    print(f"üîç SCAN DE {len(tokens)} TOKENS")
    print(f"‚è±Ô∏è  Timeframe: {timeframe}")
    print(f"üìä Filtres: {filters}")
    print("-" * 60)
    
    loader = BinanceDataLoader(
        json_cache_dir=Path("data/crypto_data_json"),
        parquet_cache_dir=Path("data/crypto_data_parquet")
    )
    
    # T√©l√©chargement parall√®le
    data = loader.download_multiple(
        symbols=tokens,
        interval=timeframe,
        days_history=30,
        max_workers=4
    )
    
    results = []
    
    for symbol, df in data.items():
        if df.empty:
            continue
        
        # Calcul indicateurs (m√™me logique que analyze_token)
        close = df['close'].values
        high = df['high'].values
        low = df['low'].values
        volume = df['volume'].values
        
        rsi = indicators_module.rsi_np(close, period=14)
        macd, signal, _ = indicators_module.macd_np(close)
        _, _, _, bb_z = indicators_module.boll_np(close, period=20)
        atr = indicators_module.atr_np(high, low, close, period=14)
        
        # Derni√®res valeurs
        current_rsi = rsi[-1]
        current_macd = macd[-1]
        current_bb_z = bb_z[-1]
        current_price = close[-1]
        current_volume = volume[-1]
        
        # Filtrage
        if filters.get("rsi_min") and current_rsi < filters["rsi_min"]:
            continue
        if filters.get("rsi_max") and current_rsi > filters["rsi_max"]:
            continue
        if filters.get("macd_positive") and current_macd < 0:
            continue
        if filters.get("bb_zscore_min") and current_bb_z < filters["bb_zscore_min"]:
            continue
        if filters.get("bb_zscore_max") and current_bb_z > filters["bb_zscore_max"]:
            continue
        
        # Score composite (exemple simple)
        score = 0
        if 40 <= current_rsi <= 60:  # RSI neutre
            score += 30
        if current_macd > 0:  # MACD bullish
            score += 25
        if -1 <= current_bb_z <= 1:  # BB dans limites
            score += 25
        if current_volume > df['volume'].mean():  # Volume √©lev√©
            score += 20
        
        results.append({
            "symbol": symbol.replace("USDC", ""),
            "price": current_price,
            "rsi": current_rsi,
            "macd": current_macd,
            "bb_zscore": current_bb_z,
            "volume_24h": current_volume,
            "score": score
        })
    
    # Tri par score
    df_results = pd.DataFrame(results)
    df_results = df_results.sort_values("score", ascending=False)
    
    print(f"\n‚úÖ {len(df_results)} tokens passent les filtres")
    print(f"\nüèÜ TOP 10:")
    print(df_results.head(10).to_string(index=False))
    
    # Sauvegarde
    output_path = Path(f"data/exports/scan_{timeframe}.csv")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df_results.to_csv(output_path, index=False)
    
    print(f"\nüíæ R√©sultats: {output_path}")
    
    return df_results


if __name__ == "__main__":
    # R√©cup√©rer top 100 tokens
    token_mgr = TokenManager()
    tokens = token_mgr.get_top_tokens(limit=100)
    
    # Scan avec filtres
    results = scan_all_tokens(
        tokens=tokens,
        timeframe="1h",
        filters={
            "rsi_min": 35,
            "rsi_max": 65,
            "macd_positive": True
        }
    )
```

---

## üìã BONNES PRATIQUES

### 1. Fr√©quence Mises √† Jour

| Type                  | Fr√©quence         | Script                   | Dur√©e      |
| --------------------- | ----------------- | ------------------------ | ---------- |
| **Top 100 tokens**    | 1x par jour       | `update_daily_tokens.py` | ~5-10 min  |
| **OHLCV 1h**          | Toutes les heures | `update_hourly.py`       | ~2-3 min   |
| **OHLCV 4h**          | Toutes les 4h     | `update_4h.py`           | ~2-3 min   |
| **Scan opportunit√©s** | 2-3x par jour     | `scan_all_tokens.py`     | ~3-5 min   |
| **Analyse token**     | √Ä la demande      | `analyze_token.py`       | ~10-30 sec |

### 2. Gestion du Cache

**‚úÖ BON:**
```python
# Utiliser cache pour analyse rapide
loader = BinanceDataLoader(
    json_cache_dir=Path("data/crypto_data_json"),
    parquet_cache_dir=Path("data/crypto_data_parquet")
)

df = loader.download_ohlcv(
    symbol="BTCUSDC",
    interval="1h",
    days_history=30,
    force_update=False  # Utilise cache si r√©cent
)
```

**‚ùå MAUVAIS:**
```python
# Force download √† chaque fois (lent!)
df = loader.download_ohlcv(
    symbol="BTCUSDC",
    interval="1h",
    days_history=30,
    force_update=True  # ‚ùå Toujours t√©l√©charger
)
```

### 3. Gestion des Erreurs

**‚úÖ BON:**
```python
try:
    df = loader.download_ohlcv(symbol="BTCUSDC", interval="1h")
    if df.empty:
        logger.warning(f"Aucune donn√©e pour BTCUSDC")
        return None
    
    # Traitement...
    
except Exception as e:
    logger.error(f"Erreur t√©l√©chargement BTCUSDC: {e}")
    return None
```

### 4. T√©l√©chargement Parall√®le

**‚úÖ BON (Rapide):**
```python
# T√©l√©chargement parall√®le (4 workers)
data = loader.download_multiple(
    symbols=top_100_tokens,
    interval="1h",
    max_workers=4  # Parall√®le
)
```

**‚ùå LENT (S√©quentiel):**
```python
# T√©l√©chargement s√©quentiel (1 par 1)
data = {}
for symbol in top_100_tokens:
    data[symbol] = loader.download_ohlcv(symbol, "1h")
    # ‚ùå Tr√®s lent!
```

### 5. Sauvegarde R√©sultats

**Format recommand√©:** Parquet (compression ZSTD)

```python
# ‚úÖ BON: Parquet compress√© (rapide, petit)
df.to_parquet(
    "data/exports/BTCUSDC_analysis.parquet",
    engine="pyarrow",
    compression="zstd",
    index=True
)

# ‚ö†Ô∏è OK: CSV (humainement lisible mais gros)
df.to_csv("data/exports/BTCUSDC_analysis.csv", index=True)

# ‚ùå √âVITER: JSON (tr√®s gros fichiers)
df.to_json("data/exports/BTCUSDC_analysis.json")
```

---

## üîÑ WORKFLOW QUOTIDIEN COMPLET

### Matin (avant trading)
```bash
# 1. Mettre √† jour top 100 tokens + OHLCV
python update_daily_tokens.py

# 2. Scan opportunit√©s
python scan_all_tokens.py

# 3. Analyser tokens int√©ressants
python analyze_token.py BTCUSDC --days 30
python analyze_token.py ETHUSDC --days 30
```

### Pendant la journ√©e
```bash
# Mise √† jour OHLCV 1h (toutes les heures via cron/scheduler)
python update_hourly.py

# Analyse token sp√©cifique √† la demande
python analyze_token.py SOLUSDC --timeframe 1h --days 7
```

### Soir (apr√®s trading)
```bash
# Scan final de la journ√©e
python scan_all_tokens.py --timeframe 4h

# Backup donn√©es importantes
python backup_data.py
```

---

## üõ†Ô∏è MAINTENANCE

### Hebdomadaire
- V√©rifier espace disque (Parquet cache)
- Nettoyer vieux fichiers exports (> 30 jours)
- V√©rifier logs erreurs

### Mensuel
- Mettre √† jour d√©pendances Python
- V√©rifier nouveaux tokens top 100
- Backup complet donn√©es

---

## üì¶ STRUCTURE FICHIERS RECOMMAND√âE

```
ThreadX/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ update_daily_tokens.py      ‚Üê Mise √† jour quotidienne
‚îÇ   ‚îú‚îÄ‚îÄ update_hourly.py             ‚Üê Mise √† jour horaire
‚îÇ   ‚îú‚îÄ‚îÄ analyze_token.py             ‚Üê Analyse token
‚îÇ   ‚îú‚îÄ‚îÄ scan_all_tokens.py           ‚Üê Scan multi-tokens
‚îÇ   ‚îî‚îÄ‚îÄ backup_data.py               ‚Üê Backup
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ crypto_data_json/            ‚Üê Cache JSON (raw)
‚îÇ   ‚îú‚îÄ‚îÄ crypto_data_parquet/         ‚Üê Cache Parquet (optimis√©)
‚îÇ   ‚îî‚îÄ‚îÄ exports/                     ‚Üê R√©sultats analyses
‚îÇ       ‚îú‚îÄ‚îÄ scans/                   ‚Üê Scans quotidiens
‚îÇ       ‚îî‚îÄ‚îÄ tokens/                  ‚Üê Analyses tokens
‚îÇ
‚îî‚îÄ‚îÄ logs/
    ‚îú‚îÄ‚îÄ update.log                   ‚Üê Log mises √† jour
    ‚îî‚îÄ‚îÄ errors.log                   ‚Üê Log erreurs
```

---

## ‚úÖ CHECKLIST QUOTIDIENNE

- [ ] Mise √† jour top 100 tokens (matin)
- [ ] T√©l√©chargement OHLCV toutes les heures
- [ ] Scan opportunit√©s 2-3x par jour
- [ ] Analyser tokens int√©ressants
- [ ] V√©rifier logs erreurs
- [ ] Backup donn√©es importantes

---

**Auteur:** ThreadX Core Team  
**Date:** 11 octobre 2025  
**Version:** 1.0
