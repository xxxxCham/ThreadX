# -*- coding: utf-8 -*-
"""
UnifiedDataHistorique_with_Indicators v2.2
- Fix: NameError `include_crypto_core` (uniformisation du nom partout UI/CLI/pipeline)
- Fix: DEFAULT_DATA_DIR (chemin Windows corrigé)
- Fallback automatique en **CLI headless** si Tkinter est indisponible
- Conversion **JSON→Parquet** incrémentale pour bougies (idempotent)
- Écriture des **indicateurs en Parquet** dans D:\\TradXPro\\indicators_db
- Bouton **"Convertir maintenant (JSON→Parquet)"** visible dans l'UI
- Option CLI `--selftest` pour test hors-ligne

Exemples CLI rapides:
  python unified_data_historique_with_indicators.py --mode convert
  python unified_data_historique_with_indicators.py --mode indicators --convert-before --include-core
  python unified_data_historique_with_indicators.py --selftest
"""

from __future__ import annotations

import os
import re
import json
import time
import queue
import logging
import threading
import platform
import argparse
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Optional, Dict, List, Tuple, Callable

import requests
import numpy as np
import pandas as pd
try:  # dotenv facultatif
    from dotenv import load_dotenv
except Exception:  # pragma: no cover
    def load_dotenv(*_, **__):
        return False
try:
    from tqdm import tqdm
except Exception:  # fallback simple si tqdm indisponible
    def tqdm(it, **_):
        return it

# =========================================================
#  Tkinter: import **optionnel** (GUI si dispo, sinon CLI)
# =========================================================
TK_AVAILABLE = False
try:  # noqa: SIM105
    import tkinter as tk  # type: ignore
    from tkinter.scrolledtext import ScrolledText  # type: ignore
    from tkinter import ttk  # type: ignore
    TK_AVAILABLE = True
except Exception:
    tk = None  # type: ignore
    ScrolledText = None  # type: ignore
    ttk = None  # type: ignore

# =========================================================
#  Chargement .env (prioritaire sur defaults ci-dessous)
# =========================================================
load_dotenv()

# =========================================================
#  Defaults alignés sur D:\\TradXPro (Windows)
# =========================================================
IS_WINDOWS = platform.system() == "Windows"

DEFAULT_OUTPUT_DIR = r"D:\\TradXPro\\best_token_DataFrame" if IS_WINDOWS else \
    "/home/user/best_token_DataFrame"
DEFAULT_JSON_PATH  = r"D:\\TradXPro\\scripts\\mise_a_jour_dataframe\\resultats_choix_des_100tokens.json" if IS_WINDOWS else \
    "/home/user/resultats_choix_des_100tokens.json"
DEFAULT_LOG_FILE   = r"D:\\TradXPro\\scripts\\mise_a_jour_dataframe\\unified_data_historique.log" if IS_WINDOWS else \
    "/home/user/unified_data_historique.log"
# IMPORTANT: le dossier Data historique **par défaut** est le même que best_token_DataFrame
DEFAULT_DATA_DIR   = r"D:\\TradXPro\\best_token_DataFrame" if IS_WINDOWS else \
    "/home/user/best_token_DataFrame"
DEFAULT_IDB_ROOT   = r"D:\\TradXPro\\indicators_db" if IS_WINDOWS else \
    "/home/user/indicators_db"

OUTPUT_DIR   = os.path.normpath(os.getenv("OUTPUT_DIR", DEFAULT_OUTPUT_DIR))
JSON_PATH    = os.path.normpath(os.getenv("JSON_PATH",  DEFAULT_JSON_PATH))
LOG_FILE     = os.path.normpath(os.getenv("LOG_FILE",   DEFAULT_LOG_FILE))
DATA_FOLDER  = os.path.normpath(os.getenv("DATA_FOLDER", DEFAULT_DATA_DIR))
INDICATORS_DB_ROOT = os.path.normpath(os.getenv("INDICATORS_DB_ROOT", DEFAULT_IDB_ROOT))

BINANCE_API_KEY = os.getenv("BINANCE_API_KEY", "")

# Ensure dirs exist
os.makedirs(os.path.dirname(JSON_PATH), exist_ok=True)
os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(DATA_FOLDER, exist_ok=True)
os.makedirs(INDICATORS_DB_ROOT, exist_ok=True)

# =========================================================
#  Logging + canal UI/CLI
# =========================================================
logger = logging.getLogger("UnifiedDataHistorique")
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
file_handler = logging.FileHandler(LOG_FILE, mode="a", encoding="utf-8")
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)
log_queue: "queue.Queue[logging.LogRecord]" = queue.Queue()

class QueueHandler(logging.Handler):
    def __init__(self, log_queue: "queue.Queue[logging.LogRecord]"):
        super().__init__()
        self.log_queue = log_queue
    def emit(self, record: logging.LogRecord) -> None:  # noqa: D401
        self.log_queue.put(record)

queue_handler = QueueHandler(log_queue)
queue_handler.setFormatter(formatter)
logger.addHandler(queue_handler)

# Console handler pour le mode CLI
console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# =========================================================
#  Paramètres opérationnels
# =========================================================
HISTORY_DAYS = 365
BINANCE_LIMIT = 1000
FILENAME_PATTERN = re.compile(r"^([A-Za-z0-9]+USDC)_([0-9mhdw]+)\.json$")
INTERVALS = ["3m", "5m", "15m", "30m", "1h"]
FORCE_UPDATE = False
MAX_WORKERS = max(4, (os.cpu_count() or 8) // 2)

# =========================================================
#  Utilitaires
# =========================================================

def interval_to_ms(interval: str) -> int:
    amount = int(re.sub(r"\D", "", interval))
    unit = re.sub(r"\d", "", interval)
    if unit == "m": return amount * 60_000
    if unit == "h": return amount * 3_600_000
    return amount * 60_000

# =========================================================
#  Sélection des tokens (sources publiques)
# =========================================================

def get_usdc_base_assets() -> List[str]:
    url = "https://api.binance.com/api/v3/exchangeInfo"
    try:
        resp = requests.get(url, timeout=10).json()
        return [s["baseAsset"].upper() for s in resp.get("symbols", []) if s["symbol"].endswith("USDC")]
    except Exception as e:  # pragma: no cover (IO)
        logger.error(f"Erreur get_usdc_base_assets: {e}")
        return []


def get_top100_marketcap_coingecko() -> List[Dict]:
    url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {"vs_currency": "usd", "order": "market_cap_desc", "per_page": 100, "page": 1}
    try:
        data = requests.get(url, params=params, timeout=10).json()
        return [{"symbol": entry["symbol"].upper(), "name": entry["name"],
                 "market_cap": entry["market_cap"], "market_cap_rank": entry["market_cap_rank"]}
                for entry in data]
    except Exception as e:  # pragma: no cover (IO)
        logger.error(f"Erreur get_top100_marketcap_coingecko: {e}")
        return []


def get_top100_volume_usdc() -> List[Dict]:
    url = "https://api.binance.com/api/v3/ticker/24hr"
    try:
        data = requests.get(url, timeout=10).json()
        usdc_volumes = [{"baseAsset": entry["symbol"].replace("USDC", ""),
                         "volume": float(entry["quoteVolume"]) }
                        for entry in data if entry["symbol"].endswith("USDC")]
        usdc_volumes.sort(key=lambda x: x["volume"], reverse=True)
        return usdc_volumes[:100]
    except Exception as e:  # pragma: no cover (IO)
        logger.error(f"Erreur get_top100_volume_usdc: {e}")
        return []


def merge_and_update_tokens(market_cap_list: List[Dict], volume_list: List[Dict]) -> List[Dict]:
    merged: Dict[str, Dict] = {}
    for mc in market_cap_list:
        sym = mc["symbol"].upper()
        merged[sym] = dict(mc, volume=0)
    for v in volume_list:
        sym = v["baseAsset"].upper()
        merged.setdefault(sym, {"symbol": sym, "name": sym, "market_cap": None, "market_cap_rank": None, "volume": 0})
        merged[sym]["volume"] = v["volume"]

    old_dict: Dict[str, Dict] = {}
    if os.path.exists(JSON_PATH):
        try:
            with open(JSON_PATH, "r", encoding="utf-8") as f:
                old = json.load(f)
            old_dict = {i["symbol"].upper(): i for i in old if isinstance(i, dict) and "symbol" in i}
        except Exception:
            pass

    old_dict.update(merged)
    final = sorted(old_dict.values(), key=lambda x: ((x.get("market_cap_rank") or 999), -x.get("volume", 0)))
    with open(JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(final, f, indent=4)
    logger.info(f"Mis à jour {len(final)} tokens dans {JSON_PATH}")
    return final

# =========================================================
#  Téléchargement OHLCV (JSON in-place)
# =========================================================

def fetch_klines(symbol: str, interval: str, start_ms: int, end_ms: int, retries: int = 3) -> List[Dict]:
    all_candles: List[Dict] = []
    cur = start_ms
    while cur < end_ms:
        params = {"symbol": symbol, "interval": interval, "startTime": cur, "endTime": end_ms, "limit": BINANCE_LIMIT}
        for attempt in range(retries):
            try:
                data = requests.get("https://api.binance.com/api/v3/klines", params=params, timeout=10).json()
                if isinstance(data, dict) and "code" in data:
                    raise ValueError(f"API error: {data}")
                break
            except Exception as e:  # pragma: no cover (IO)
                logger.warning(f"fetch_klines retry {attempt+1}/{retries}: {e}")
                time.sleep(5)
        else:
            return all_candles
        if not data:
            break
        all_candles.extend([
            {"timestamp": c[0], "open": c[1], "high": c[2], "low": c[3], "close": c[4], "volume": c[5],
             "extra": {"close_time": c[6], "quote_asset_volume": c[7], "trades_count": c[8],
                       "taker_buy_base_volume": c[9], "taker_buy_quote_volume": c[10]}}
            for c in data
        ])
        cur = data[-1][0] + 1
        time.sleep(0.2)
    return all_candles


def download_ohlcv(tokens: List[str], usdc_symbols: set, force_update: bool = False, progress_callback: Optional[Callable[[float,int,int],None]] = None) -> None:
    now = int(time.time() * 1000)
    start = now - HISTORY_DAYS * 86_400_000
    total_tokens = len(tokens)
    treated = 0
    for token in tqdm(tokens, desc="Téléchargement Tokens"):
        symbol = f"{token}USDC"
        if symbol not in usdc_symbols:
            continue
        for interval in INTERVALS:
            fname = f"{symbol}_{interval}.json"
            fpath = os.path.join(DATA_FOLDER, fname)
            if os.path.exists(fpath):
                recent = (time.time() - os.path.getmtime(fpath)) < 86_400
                if recent and not force_update:
                    logger.info(f"{fname} récent (<1j), skip.")
                    continue
                try:
                    with open(fpath, "r", encoding="utf-8") as f:
                        old = json.load(f)
                    ts = [c["timestamp"] for c in old]
                    if sorted(ts) != ts or len(set(ts)) != len(ts):
                        logger.warning(f"{fname} corrompu → re-download complet.")
                        data = fetch_klines(symbol, interval, start, now)
                    else:
                        last_ts = max(ts) if ts else start
                        upd_start = last_ts + interval_to_ms(interval)
                        if upd_start >= now:
                            logger.info(f"{fname} à jour, skip.")
                            continue
                        data = fetch_klines(symbol, interval, upd_start, now)
                        data = (old + data) if data else old
                        data.sort(key=lambda x: x["timestamp"])
                except json.JSONDecodeError:
                    logger.error(f"{fname} JSON invalide → re-download complet.")
                    data = fetch_klines(symbol, interval, start, now)
            else:
                data = fetch_klines(symbol, interval, start, now)

            if data:
                with open(fpath, "w", encoding="utf-8") as f:
                    json.dump(data, f, indent=2)
                logger.info(f"Enregistré/Mis à jour {len(data)} bougies pour {fname}")
        treated += 1
        if progress_callback:
            progress_callback(treated / total_tokens * 100, treated, total_tokens)

# =========================================================
#  Vérification & complétion (JSON in-place)
# =========================================================

def detect_missing(candles: List[Dict], interval: str, start_ms: int, end_ms: int) -> List[Tuple[int,int]]:
    if not candles:
        return [(start_ms, end_ms)]
    candles.sort(key=lambda x: x["timestamp"])
    step = interval_to_ms(interval)
    missing: List[Tuple[int,int]] = []
    first = candles[0]["timestamp"]
    if first > start_ms:
        missing.append((start_ms, first - 1))
    for i in range(len(candles) - 1):
        a, b = candles[i]["timestamp"], candles[i + 1]["timestamp"]
        if b - a > step:
            missing.append((a + step, b - 1))
    last = candles[-1]["timestamp"]
    if last < end_ms:
        missing.append((last + step, end_ms))
    return [g for g in missing if g[0] < g[1]]


def verify_and_complete(progress_callback: Optional[Callable[[float,int,int],None]] = None) -> None:
    now = datetime.utcnow()
    start_ms = int((now - timedelta(days=HISTORY_DAYS)).timestamp() * 1000)
    end_ms   = int(now.timestamp() * 1000)
    files = [f for f in os.listdir(DATA_FOLDER) if FILENAME_PATTERN.match(f)]
    total = len(files)
    done = 0
    for file in tqdm(files, desc="Vérification Fichiers"):
        m = FILENAME_PATTERN.match(file)
        symbol, interval = m.groups()  # type: ignore[union-attr]
        fpath = os.path.join(DATA_FOLDER, file)
        try:
            with open(fpath, "r", encoding="utf-8") as f:
                data = json.load(f)
            ts = [c["timestamp"] for c in data]
            if sorted(ts) != ts or len(set(ts)) != len(ts):
                logger.warning(f"{file} corrompu → re-complétion totale.")
                data = []
        except json.JSONDecodeError:
            logger.error(f"{file} invalide → re-complétion totale.")
            data = []
        gaps = detect_missing(data, interval, start_ms, end_ms)
        if gaps:
            add: List[Dict] = []
            for a, b in gaps:
                add.extend(fetch_klines(symbol, interval, a, b))
            if add:
                merged = list({c["timestamp"]: c for c in (data + add)}.values())
                merged.sort(key=lambda x: x["timestamp"])  # type: ignore[index]
                with open(fpath, "w", encoding="utf-8") as f:
                    json.dump(merged, f, indent=2)
                logger.info(f"Complété {file} (+{len(add)} bougies)")
        done += 1
        if progress_callback:
            progress_callback(done / total * 100, done, total)

# =========================================================
#  Conversion JSON→Parquet (bougies historiques)
# =========================================================

def _json_to_df(json_path: str) -> Optional[pd.DataFrame]:
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception:
        return None
    if not data:
        return None
    df = pd.DataFrame(data)
    for c in ["open","high","low","close","volume"]:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms", utc=True)
    df = df.set_index("timestamp").sort_index()
    return df


def json_candles_to_parquet(json_path: str, out_dir: Optional[str] = None, compression: str = "snappy") -> Optional[str]:
    root, _ = os.path.splitext(os.path.basename(json_path))
    out_path = os.path.join(out_dir or os.path.dirname(json_path), root + ".parquet")
    try:
        json_mtime = os.path.getmtime(json_path)
        if os.path.exists(out_path):
            pq_mtime = os.path.getmtime(out_path)
            if pq_mtime >= json_mtime:  # déjà à jour
                return out_path
    except Exception:
        pass

    df = _json_to_df(json_path)
    if df is None or df.empty:
        return None
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df.to_parquet(out_path, engine="pyarrow", compression=compression)
    logger.info(f"Parquet écrit: {out_path}")
    return out_path


def convert_all_candles(data_dir: str, out_dir: Optional[str] = None, workers: int = MAX_WORKERS, progress_callback: Optional[Callable[[float,int,int],None]] = None) -> int:
    files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if FILENAME_PATTERN.match(f)]
    total = len(files)
    written = 0

    def _one(p: str) -> int:
        try:
            outp = json_candles_to_parquet(p, out_dir)
            return 1 if outp else 0
        except Exception as e:  # pragma: no cover (IO)
            logger.error(f"Parquet conversion failed for {p}: {e}")
            return 0

    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = [ex.submit(_one, p) for p in files]
        for i, fut in enumerate(as_completed(futures), 1):
            try:
                written += fut.result()
            except Exception as e:  # pragma: no cover
                logger.error(f"convert_all_candles error: {e}")
            if progress_callback:
                progress_callback(i / total * 100, i, total)
    logger.info(f"Parquet (bougies) écrits/validés: {written}")
    return written

# =========================================================
#  Indicateurs (numpy/pandas) + écriture Parquet
# =========================================================

def _ewm(x: np.ndarray, span: int) -> np.ndarray:
    x = np.asarray(x, dtype=np.float64)
    if x.size == 0: return np.array([], dtype=np.float64)
    out = np.empty_like(x, dtype=np.float64)
    out[0] = x[0]
    if span <= 1:
        out[:] = x
        return out
    a = 2.0 / (span + 1.0)
    for i in range(1, len(x)):
        out[i] = a * x[i] + (1 - a) * out[i - 1]
    return out


def ema_np(arr: np.ndarray, span: int) -> np.ndarray:
    return _ewm(arr, span)


def atr_np(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> np.ndarray:
    prev = np.concatenate(([close[0]], close[:-1]))
    tr = np.maximum(high - low, np.maximum(np.abs(high - prev), np.abs(low - prev)))
    return _ewm(tr, period)


def boll_np(close: np.ndarray, period: int = 20, std: float = 2.0):
    ma  = _ewm(close, period)
    var = _ewm((close - ma) ** 2, period)
    sd  = np.sqrt(np.maximum(var, 1e-12))
    upper = ma + std * sd
    lower = ma - std * sd
    z = (close - ma) / sd
    return lower, ma, upper, z


def rsi_np(close: np.ndarray, period: int = 14) -> np.ndarray:
    if close.size == 0: return np.array([], dtype=np.float64)
    delta = np.diff(close, prepend=close[0])
    gain = np.where(delta > 0, delta, 0.0)
    loss = np.where(delta < 0, -delta, 0.0)
    avg_gain = _ewm(gain, period)
    avg_loss = _ewm(loss, period)
    rs = np.divide(avg_gain, np.maximum(avg_loss, 1e-12))
    rsi = 100.0 - (100.0 / (1.0 + rs))
    return rsi


def macd_np(close: np.ndarray, fast: int = 12, slow: int = 26, signal: int = 9):
    ema_fast = ema_np(close, fast)
    ema_slow = ema_np(close, slow)
    macd = ema_fast - ema_slow
    sig = ema_np(macd, signal)
    hist = macd - sig
    return macd, sig, hist


def vwap_np(close: np.ndarray, high: np.ndarray, low: np.ndarray, volume: np.ndarray, window: int = 96):
    typical = (high + low + close) / 3.0
    vol_ema = ema_np(volume, window)
    pv_ema  = ema_np(typical * volume, window)
    vwap = np.divide(pv_ema, np.maximum(vol_ema, 1e-12))
    return vwap


def obv_np(close: np.ndarray, volume: np.ndarray):
    obv = np.zeros_like(close, dtype=np.float64)
    for i in range(1, close.size):
        if close[i] > close[i-1]: obv[i] = obv[i-1] + volume[i]
        elif close[i] < close[i-1]: obv[i] = obv[i-1] - volume[i]
        else: obv[i] = obv[i-1]
    return obv


def vortex_df(highs: np.ndarray, lows: np.ndarray, closes: np.ndarray, period: int = 14) -> pd.DataFrame:
    n = len(closes)
    if n == 0:
        return pd.DataFrame({"vi_plus": [], "vi_minus": []})
    h = highs.astype(np.float64); l = lows.astype(np.float64); c = closes.astype(np.float64)
    prev_h = np.roll(h, 1); prev_l = np.roll(l, 1); prev_c = np.roll(c, 1)
    prev_h[0] = h[0]; prev_l[0] = l[0]; prev_c[0] = c[0]
    vm_plus  = np.abs(h - prev_l)
    vm_minus = np.abs(l - prev_h)
    tr = np.maximum(h - l, np.maximum(np.abs(h - prev_c), np.abs(l - prev_c)))
    vm_p_sum = pd.Series(vm_plus, dtype="float64").rolling(window=period, min_periods=period).sum()
    vm_m_sum = pd.Series(vm_minus, dtype="float64").rolling(window=period, min_periods=period).sum()
    tr_sum   = pd.Series(tr, dtype="float64").rolling(window=period, min_periods=period).sum()
    vi_plus  = (vm_p_sum / tr_sum).to_numpy()
    vi_minus = (vm_m_sum / tr_sum).to_numpy()
    return pd.DataFrame({"vi_plus": vi_plus, "vi_minus": vi_minus})

# Préférer le Parquet si dispo (lecture des bougies)

def _to_df(json_or_parquet_file: str) -> Optional[pd.DataFrame]:
    path = os.path.normpath(json_or_parquet_file)
    root, ext = os.path.splitext(path)
    parquet_path = root + ".parquet"
    try:
        if ext.lower() == ".parquet" and os.path.exists(path):
            return pd.read_parquet(path)
        if os.path.exists(parquet_path):
            return pd.read_parquet(parquet_path)
        # fallback JSON
        return _json_to_df(path)
    except Exception as e:  # pragma: no cover (IO)
        logger.error(f"_to_df read error for {path}: {e}")
        return None


def _write_parquet(df_out: pd.DataFrame, out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    df_out.to_parquet(out_path, engine="pyarrow")


def generate_indicators_for_file(json_path: str, out_root: str, indicators_cfg: dict, include_crypto_core: bool = False):
    """
    DougStream: Calcule et écrit les indicateurs techniques pour un fichier OHLCV.

    - Charge les données via `read_candles()` (Parquet-first).
    - Normalise le DataFrame (index en UTC).
    - Ne calcule les indicateurs que si le DataFrame n’est pas vide.
    - Pour chaque indicateur, écrit un fichier Parquet distinct sous
      `{out_root}/{symbole}/{timeframe}/{nom_indicateur}/{nom_indicateur}_{signature}.parquet`.
    - L’argument `include_crypto_core` permet d’activer les indicateurs de base
      (RSI, MACD, VWAP, OBV, EMA longue) en plus des indicateurs listés dans `indicators_cfg`.
    """
    base = os.path.basename(json_path)
    # Supporte entrée .json ou .parquet (même pattern de nom)
    base = base[:-8] + ".json" if base.endswith(".parquet") else base
    m = FILENAME_PATTERN.match(base)
    if not m:
        return {"file": json_path, "written": 0}
    symbol, interval = m.groups()
    df = _to_df(json_path)
    if df is None or df.empty:
        return {"file": json_path, "written": 0}

    close = df["close"].to_numpy(np.float64)
    high  = df["high"].to_numpy(np.float64)
    low   = df["low"].to_numpy(np.float64)
    vol   = df["volume"].to_numpy(np.float64)

    out_dir = os.path.join(out_root, symbol.replace("USDC",""), interval)
    written = 0

    # Bollinger
    for p in indicators_cfg.get("bollinger", {}).get("periods", [20]):
        for s in indicators_cfg.get("bollinger", {}).get("stds", [2.0]):
            lo, mid, up, z = boll_np(close, int(p), float(s))
            dfb = pd.DataFrame({"mid": mid, "upper": up, "lower": lo, "z": z}, index=df.index)
            out = os.path.join(out_dir, f"bollinger_period{int(p)}_std{float(s)}.parquet")
            _write_parquet(dfb, out); written += 1

    # ATR
    for a in indicators_cfg.get("atr", {}).get("periods", [14]):
        atr = atr_np(high, low, close, int(a))
        dfa = pd.DataFrame({"atr": atr}, index=df.index)
        out = os.path.join(out_dir, f"atr_period{int(a)}.parquet")
        _write_parquet(dfa, out); written += 1

    # EMA
    for e in indicators_cfg.get("ema", {}).get("periods", []):
        ema = ema_np(close, int(e))
        dfe = pd.DataFrame({"ema": ema}, index=df.index)
        out = os.path.join(out_dir, f"ema_period{int(e)}.parquet")
        _write_parquet(dfe, out); written += 1

    # RSI/MACD/VWAP/OBV optionnels
    if include_crypto_core:
        for rp in indicators_cfg.get("rsi", {}).get("periods", [14, 21]):
            r = rsi_np(close, int(rp))
            dfr = pd.DataFrame({"rsi": r}, index=df.index)
            out = os.path.join(out_dir, f"rsi_period{int(rp)}.parquet")
            _write_parquet(dfr, out); written += 1
        for f_ in indicators_cfg.get("macd", {}).get("fast", [12]):
            for sl in indicators_cfg.get("macd", {}).get("slow", [26]):
                for sg in indicators_cfg.get("macd", {}).get("signal", [9]):
                    m_, s_, h_ = macd_np(close, int(f_), int(sl), int(sg))
                    dfm = pd.DataFrame({"macd": m_, "signal": s_, "hist": h_}, index=df.index)
                    out = os.path.join(out_dir, f"macd_fast{int(f_)}_signal{int(sg)}_slow{int(sl)}.parquet")
                    _write_parquet(dfm, out); written += 1
        for w in indicators_cfg.get("vwap", {}).get("windows", [96]):
            v = vwap_np(close, high, low, vol, int(w))
            dfv = pd.DataFrame({"vwap": v}, index=df.index)
            out = os.path.join(out_dir, f"vwap_window{int(w)}.parquet")
            _write_parquet(dfv, out); written += 1
        obv = obv_np(close, vol)
        dfo = pd.DataFrame({"obv": obv}, index=df.index)
        out = os.path.join(out_dir, "obv.parquet")
        _write_parquet(dfo, out); written += 1

    # Vortex
    if "vortex" in indicators_cfg:
        periods = indicators_cfg.get("vortex", {}).get("periods", [14])
        for p in periods:
            dfvtx = vortex_df(high, low, close, int(p))
            out = os.path.join(out_dir, f"vortex_period{int(p)}.parquet")
            _write_parquet(dfvtx, out); written += 1

    return {"file": json_path, "written": written}


def generate_all_indicators(data_dir: str, out_dir: str, indicators_cfg: dict,
                            include_crypto_core: bool = False,
                            workers: int = MAX_WORKERS, progress_callback: Optional[Callable[[float,int,int],None]] = None) -> None:
    files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if FILENAME_PATTERN.match(f)]
    total = len(files)
    written_total = 0
    with ThreadPoolExecutor(max_workers=workers) as ex:
        futures = [ex.submit(generate_indicators_for_file, f, out_dir, indicators_cfg, include_crypto_core) for f in files]
        for i, fut in enumerate(as_completed(futures), 1):
            try:
                res = fut.result()
                written_total += res.get("written", 0)
            except Exception as e:  # pragma: no cover
                logger.error(f"Erreur génération indicateurs: {e}")
            if progress_callback:
                progress_callback(i / total * 100, i, total)
    logger.info(f"Indicateurs écrits: {written_total} fichiers parquet.")

# =========================================================
#  PIPELINE partagé (UI/CLI)
# =========================================================

def run_pipeline(mode: str, include_crypto_core: bool, convert_before: bool, force_update: bool = False,
                 progress_cb: Optional[Callable[[float,int,int],None]] = None) -> None:
    # 1) Sélection
    if mode in ["full", "select"]:
        usdc_assets = get_usdc_base_assets()
        mc_list = get_top100_marketcap_coingecko()
        vol_list = get_top100_volume_usdc()
        tokens_data = merge_and_update_tokens(mc_list, vol_list)
        tokens = [t["symbol"] for t in tokens_data]
    else:
        with open(JSON_PATH, "r", encoding="utf-8") as f:
            tokens_data = json.load(f)
        tokens = [t["symbol"] for t in tokens_data]
        usdc_assets = get_usdc_base_assets()

    # 2) Download
    if mode in ["full", "download"]:
        download_ohlcv(tokens, set(f"{t}USDC" for t in usdc_assets), force_update, progress_cb)

    # 3) Vérification
    if mode in ["full", "verify"]:
        verify_and_complete(progress_cb)

    # 4) Conversion JSON→Parquet (bougies)
    if mode in ["full", "convert"] or (mode == "indicators" and convert_before):
        convert_all_candles(DATA_FOLDER, None, MAX_WORKERS, progress_cb)

    # 5) Indicateurs
    if mode in ["full", "indicators"]:
        indicators = {
            "bollinger": {"periods": [10, 20, 50], "stds": [1.5, 2.0, 2.5]},
            "atr": {"periods": [14, 21]},
            "ema": {"periods": [20, 50, 200]},
            "vortex": {"periods": [14, 21]},
        }
        generate_all_indicators(DATA_FOLDER, INDICATORS_DB_ROOT, indicators, include_crypto_core, MAX_WORKERS, progress_cb)

    logger.info("Processus terminé.")

# =========================================================
#  UI Tkinter (seulement si disponible)
# =========================================================

if TK_AVAILABLE:
    class UnifiedApp(tk.Tk):  # type: ignore[misc]
        def __init__(self):
            super().__init__()
            self.title("Unified Data Historique + Indicators — CONVERT BTN")
            self.configure(bg="#1e1e1e")
            tk.Label(self, text="Étapes : Sélection > Download > Vérification > Conversion > Indicateurs",
                     bg="#1e1e1e", fg="#ffffff").pack(pady=10)

            self.mode_var = tk.StringVar(value="full")
            modes = [
                ("Complet", "full"),
                ("Sélection", "select"),
                ("Download", "download"),
                ("Vérification", "verify"),
                ("Convertir bougies (JSON→Parquet)", "convert"),
                ("Indicateurs seulement", "indicators"),
            ]
            for text, val in modes:
                tk.Radiobutton(self, text=text, variable=self.mode_var, value=val, bg="#1e1e1e", fg="#ffffff",
                               selectcolor="#333333").pack(anchor="w")

            # === Bloc VISUEL Conversion très visible ===
            self.chk_crypto_core = tk.BooleanVar(value=True)
            tk.Checkbutton(self, text="Inclure RSI/MACD/VWAP/OBV/EMA200", variable=self.chk_crypto_core,
                           bg="#1e1e1e", fg="#ffffff", selectcolor="#333333").pack(anchor="w", padx=4, pady=4)

            self.chk_convert_before = tk.BooleanVar(value=True)
            conv_frame = tk.LabelFrame(self, text="Conversion bougies (JSON→Parquet)", fg="#ffffff", bg="#2a2a2a",
                                       labelanchor="n", padx=8, pady=6)
            conv_frame.pack(fill=tk.X, padx=8, pady=8)

            tk.Checkbutton(conv_frame,
                           text="Convertir bougies JSON→Parquet AVANT le calcul des indicateurs",
                           variable=self.chk_convert_before,
                           bg="#2a2a2a", fg="#ffffff", selectcolor="#333333").pack(anchor="w", pady=4)

            self.btn_convert_now = tk.Button(conv_frame,
                                             text="➡ Convertir maintenant (JSON→Parquet)",
                                             command=self.convert_now,
                                             font=("Segoe UI", 11, "bold"),
                                             bg="#006699", fg="#ffffff",
                                             padx=10, pady=6)
            self.btn_convert_now.pack(fill=tk.X, pady=4)

            # Boutons contrôle
            btn = tk.Frame(self, bg="#1e1e1e"); btn.pack(pady=5)
            self.play_btn  = tk.Button(btn, text="Play", command=self.start_process, bg="#333333", fg="#ffffff")
            self.pause_btn = tk.Button(btn, text="Pause", command=self.pause_process, state=tk.DISABLED, bg="#333333", fg="#ffffff")
            self.stop_btn  = tk.Button(btn, text="Arrêter", command=self.stop_process, state=tk.DISABLED, bg="#333333", fg="#ffffff")
            self.play_btn.pack(side=tk.LEFT, padx=5); self.pause_btn.pack(side=tk.LEFT, padx=5); self.stop_btn.pack(side=tk.LEFT, padx=5)

            self.progress_label = tk.Label(self, text="Progression : 0/0", bg="#1e1e1e", fg="#ffffff"); self.progress_label.pack(pady=5)
            self.progress_bar   = ttk.Progressbar(self, orient="horizontal", length=420, mode="determinate"); self.progress_bar.pack(pady=5)

            self.log_text = ScrolledText(self, state="disabled", height=20, width=88, bg="#000000", fg="#d3d3d3", insertbackground="#ffffff")
            self.log_text.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
            self.log_text.tag_config("INFO", foreground="#d3d3d3")
            self.log_text.tag_config("WARNING", foreground="#ffa500")
            self.log_text.tag_config("ERROR", foreground="#ff0000")

            self.stop_flag = self.pause_flag = False
            self.process_thread = None
            self.after(100, self.poll_log_queue)
            self.protocol("WM_DELETE_WINDOW", self.on_close)

        def poll_log_queue(self):
            while True:
                try:
                    record = log_queue.get_nowait()
                except queue.Empty:
                    break
                msg = queue_handler.format(record)
                self.log_text.config(state="normal")
                self.log_text.insert(tk.END, msg + "\n", record.levelname)
                self.log_text.config(state="disabled")
                self.log_text.yview(tk.END)
            self.after(100, self.poll_log_queue)

        def update_progress(self, percent: float, treated: int, total: int):
            self.progress_bar["value"] = percent
            self.progress_label.config(text=f"Traité : {treated}/{total}")
            self.update_idletasks()

        def convert_now(self):
            # Lance UNIQUEMENT la conversion de bougies depuis le gros bouton
            self.mode_var.set("convert")
            self.start_process()

        def start_process(self):
            if self.process_thread and self.process_thread.is_alive():
                if self.pause_flag:
                    self.pause_flag = False
                    logger.info("Reprise.")
                    self.play_btn.config(state=tk.DISABLED); self.pause_btn.config(state=tk.NORMAL); self.stop_btn.config(state=tk.NORMAL)
                    return
            self.stop_flag = self.pause_flag = False
            self.play_btn.config(state=tk.DISABLED)
            self.pause_btn.config(state=tk.NORMAL)
            self.stop_btn.config(state=tk.NORMAL)
            self.progress_bar["value"] = 0
            self.progress_label.config(text="Progression : 0/0")
            self.process_thread = threading.Thread(target=self.run_process, daemon=True)
            self.process_thread.start()

        def pause_process(self):
            self.pause_flag = True
            logger.info("Pause.")
            self.pause_btn.config(state=tk.DISABLED); self.play_btn.config(state=tk.NORMAL)

        def stop_process(self):
            self.stop_flag = True
            self.pause_flag = False
            logger.warning("Arrêt.")
            self.play_btn.config(state=tk.DISABLED); self.pause_btn.config(state=tk.DISABLED); self.stop_btn.config(state=tk.DISABLED)

        def on_close(self):
            self.stop_flag = True
            self.pause_flag = False
            if self.process_thread:
                self.process_thread.join(timeout=5)
            self.destroy()

        def run_process(self):
            mode = self.mode_var.get()
            include_crypto_core = self.chk_crypto_core.get()
            convert_before = self.chk_convert_before.get()
            def progress_cb(percent: float, treated: int, total: int):
                self.after(0, lambda: self.update_progress(percent, treated, total))
            run_pipeline(mode, include_crypto_core, convert_before, FORCE_UPDATE, progress_cb)
            self.after(0, lambda: (self.play_btn.config(state=tk.NORMAL), self.pause_btn.config(state=tk.DISABLED), self.stop_btn.config(state=tk.DISABLED)))

# =========================================================
#  Tests offline (sans réseau) — `--selftest`
# =========================================================

def _selftest(tmpdir: str) -> None:
    import tempfile
    import shutil
    from pathlib import Path

    td = Path(tmpdir)
    data_dir = td / "data"; data_dir.mkdir(parents=True, exist_ok=True)
    out_dir  = td / "out"; out_dir.mkdir(parents=True, exist_ok=True)

    # Mini JSON OHLCV synthétique
    js = [
        {"timestamp": 1_700_000_000_000, "open": "10", "high": "11", "low": "9",  "close": "10.5", "volume": "100"},
        {"timestamp": 1_700_000_180_000, "open": "10.5", "high": "12", "low": "10", "close": "11.5", "volume": "120"},
        {"timestamp": 1_700_000_360_000, "open": "11.5", "high": "12", "low": "11", "close": "11.2", "volume": "80"},
    ]
    fjson = data_dir / "TESTUSDC_3m.json"
    with open(fjson, "w", encoding="utf-8") as f:
        json.dump(js, f)

    # 1) Conversion JSON→Parquet
    outp = json_candles_to_parquet(str(fjson))
    assert outp and os.path.exists(outp), "Parquet non écrit"
    df = pd.read_parquet(outp)
    assert {"open","high","low","close","volume"}.issubset(df.columns), "Colonnes manquantes"

    # 2) Vortex calcul
    vdf = vortex_df(df["high"].values, df["low"].values, df["close"].values, period=2)
    assert {"vi_plus","vi_minus"}.issubset(vdf.columns), "Vortex colonnes manquantes"

    # 3) Génération indicateurs sur fichier unique
    cfg = {"ema": {"periods": [2]}, "vortex": {"periods": [2]}}
    res = generate_indicators_for_file(str(fjson), str(out_dir), cfg, include_crypto_core=False)
    assert res["written"] >= 2, "Indicateurs non générés"

    shutil.rmtree(td)

# =========================================================
#  Entrée principale (UI si Tkinter, sinon CLI)
# =========================================================

def _cli_main() -> None:
    parser = argparse.ArgumentParser(description="Unified Data Historique + Indicators (CLI headless)")
    parser.add_argument("--mode", choices=["full","select","download","verify","convert","indicators"], default="full")
    parser.add_argument("--include-core", action="store_true", help="Inclure RSI/MACD/VWAP/OBV/EMA200")
    parser.add_argument("--convert-before", action="store_true", help="Convertir JSON→Parquet avant indicateurs (si --mode indicators)")
    parser.add_argument("--force-update", action="store_true", help="Forcer re-download OHLCV")
    parser.add_argument("--selftest", action="store_true", help="Exécuter les tests offline")
    args = parser.parse_args()

    if args.selftest:
        import tempfile
        _selftest(tempfile.mkdtemp(prefix="txp_selftest_"))
        print("SELFTEST OK")
        return

    def progress_cb(p: float, t: int, total: int):
        print(f"Progress {p:5.1f}%  ({t}/{total})")
    run_pipeline(args.mode, args.include_core, args.convert_before, args.force_update, progress_cb)


if __name__ == "__main__":
    if TK_AVAILABLE:
        app = UnifiedApp()  # type: ignore[name-defined]
        app.mainloop()
    else:
        _cli_main()
