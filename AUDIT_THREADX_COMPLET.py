#!/usr/bin/env python3
"""
Audit complet ThreadX - Phase 1: Pr√©paration et Auditing
=========================================================

Ce script impl√©mente un audit exhaustif du projet ThreadX selon les bonnes pratiques
pour les applications de trading quantitatif.

Cat√©gories d'audit:
1. Logic Errors (erreurs de logique trading)
2. Code Duplication (duplication de code)
3. Structural Issues (probl√®mes structurels)
4. Security Issues (vuln√©rabilit√©s)
5. Performance Issues (probl√®mes de performance)

Auteur: ThreadX Audit System
Date: Octobre 2025
"""

import ast
import json
import os
import re
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Set, Tuple

# Configuration
PROJECT_ROOT = Path(__file__).parent
SRC_DIR = PROJECT_ROOT / "src" / "threadx"
TESTS_DIR = PROJECT_ROOT / "tests"
REPORT_FILE = PROJECT_ROOT / "AUDIT_THREADX_FINDINGS.json"
MARKDOWN_REPORT = PROJECT_ROOT / "AUDIT_THREADX_REPORT.md"

# Seuils de qualit√©
DUPLICATION_THRESHOLD = 10  # % max de duplication acceptable
COMPLEXITY_THRESHOLD = 10  # Complexit√© cyclomatique max par fonction
MIN_DOCSTRING_COVERAGE = 70  # % minimum de fonctions document√©es


@dataclass
class Finding:
    """Repr√©sente une d√©couverte d'audit"""

    category: str  # 'logic', 'duplication', 'structural', 'security', 'performance'
    severity: str  # 'critical', 'high', 'medium', 'low'
    file_path: str
    line_number: int
    description: str
    recommendation: str
    code_snippet: str = ""


@dataclass
class AuditStats:
    """Statistiques globales de l'audit"""

    total_files: int = 0
    total_lines: int = 0
    total_functions: int = 0
    total_classes: int = 0
    duplicated_lines: int = 0
    duplication_percentage: float = 0.0
    findings_by_severity: Dict[str, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    findings_by_category: Dict[str, int] = field(
        default_factory=lambda: defaultdict(int)
    )


class ThreadXAuditor:
    """Auditeur principal pour ThreadX"""

    def __init__(self):
        self.findings: List[Finding] = []
        self.stats = AuditStats()
        self.code_blocks: Dict[str, List[Tuple[str, int]]] = defaultdict(list)
        self.imports_by_file: Dict[str, Set[str]] = {}

    def run_full_audit(self) -> None:
        """Ex√©cute l'audit complet"""
        print("üîç D√©marrage de l'audit complet ThreadX...\n")

        # Phase 1: Collecte des fichiers
        python_files = self._collect_python_files()
        print(f"üìÅ {len(python_files)} fichiers Python trouv√©s\n")

        # Phase 2: Analyse de chaque fichier
        for file_path in python_files:
            self._analyze_file(file_path)

        # Phase 3: Audits sp√©cifiques
        print("üî¨ Audit des erreurs logiques de trading...")
        self._audit_trading_logic()

        print("üîç D√©tection de la duplication de code...")
        self._detect_code_duplication()

        print("üèóÔ∏è Analyse structurelle...")
        self._audit_structure()

        print("üõ°Ô∏è Audit de s√©curit√©...")
        self._audit_security()

        print("‚ö° Audit de performance GPU...")
        self._audit_gpu_performance()

        # Phase 4: G√©n√©ration des rapports
        print("\nüìä G√©n√©ration des rapports...")
        self._generate_reports()

        print(f"\n‚úÖ Audit termin√©! {len(self.findings)} probl√®mes d√©tect√©s.")
        print(f"üìÑ Rapport JSON: {REPORT_FILE}")
        print(f"üìÑ Rapport Markdown: {MARKDOWN_REPORT}")

    def _collect_python_files(self) -> List[Path]:
        """Collecte tous les fichiers Python du projet"""
        files = []
        for directory in [SRC_DIR, TESTS_DIR]:
            if directory.exists():
                files.extend(directory.rglob("*.py"))

        # Exclure les fichiers g√©n√©r√©s et caches
        exclude_patterns = ["__pycache__", ".egg-info", "build", "dist"]
        return [f for f in files if not any(p in str(f) for p in exclude_patterns)]

    def _analyze_file(self, file_path: Path) -> None:
        """Analyse un fichier Python"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.split("\n")

            self.stats.total_files += 1
            self.stats.total_lines += len(lines)

            # Parse AST
            try:
                tree = ast.parse(content, filename=str(file_path))
                self._analyze_ast(tree, file_path, content)
            except SyntaxError as e:
                self._add_finding(
                    category="structural",
                    severity="critical",
                    file_path=str(file_path),
                    line_number=e.lineno or 0,
                    description=f"Erreur de syntaxe: {e.msg}",
                    recommendation="Corriger l'erreur de syntaxe imm√©diatement",
                )

            # Analyse des imports
            self._analyze_imports(file_path, content)

            # Stockage des blocs de code pour d√©tection de duplication
            self._store_code_blocks(file_path, lines)

        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de l'analyse de {file_path}: {e}")

    def _analyze_ast(self, tree: ast.AST, file_path: Path, content: str) -> None:
        """Analyse l'AST pour d√©tecter les probl√®mes"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                self.stats.total_functions += 1
                self._check_function_complexity(node, file_path)
                self._check_function_docstring(node, file_path)

            elif isinstance(node, ast.ClassDef):
                self.stats.total_classes += 1
                self._check_class_structure(node, file_path)

    def _check_function_complexity(
        self, node: ast.FunctionDef, file_path: Path
    ) -> None:
        """V√©rifie la complexit√© cyclomatique d'une fonction"""
        complexity = self._calculate_complexity(node)

        if complexity > COMPLEXITY_THRESHOLD:
            self._add_finding(
                category="structural",
                severity="medium",
                file_path=str(file_path),
                line_number=node.lineno,
                description=f"Fonction '{node.name}' trop complexe (complexit√©: {complexity})",
                recommendation=f"Refactoriser en fonctions plus petites. Cible: <{COMPLEXITY_THRESHOLD}",
            )

    def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """Calcule la complexit√© cyclomatique (McCabe)"""
        complexity = 1
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1
        return complexity

    def _check_function_docstring(self, node: ast.FunctionDef, file_path: Path) -> None:
        """V√©rifie la pr√©sence de docstrings"""
        if not ast.get_docstring(node) and not node.name.startswith("_"):
            self._add_finding(
                category="structural",
                severity="low",
                file_path=str(file_path),
                line_number=node.lineno,
                description=f"Fonction publique '{node.name}' sans docstring",
                recommendation="Ajouter une docstring d√©crivant le comportement",
            )

    def _check_class_structure(self, node: ast.ClassDef, file_path: Path) -> None:
        """V√©rifie la structure des classes"""
        methods = [n for n in node.body if isinstance(n, ast.FunctionDef)]

        # Classes trop grandes
        if len(methods) > 20:
            self._add_finding(
                category="structural",
                severity="medium",
                file_path=str(file_path),
                line_number=node.lineno,
                description=f"Classe '{node.name}' trop grande ({len(methods)} m√©thodes)",
                recommendation="Consid√©rer de diviser en classes plus petites",
            )

    def _analyze_imports(self, file_path: Path, content: str) -> None:
        """Analyse les imports pour d√©tecter les duplications"""
        imports = set()
        for line in content.split("\n"):
            if line.strip().startswith(("import ", "from ")):
                imports.add(line.strip())

        self.imports_by_file[str(file_path)] = imports

    def _store_code_blocks(self, file_path: Path, lines: List[str]) -> None:
        """Stocke les blocs de code pour d√©tection de duplication"""
        # Blocs de 5+ lignes non vides
        for i in range(len(lines) - 4):
            block_lines = [
                l.strip()
                for l in lines[i : i + 5]
                if l.strip() and not l.strip().startswith("#")
            ]
            if len(block_lines) >= 5:
                block_hash = hash(tuple(block_lines))
                self.code_blocks[str(block_hash)].append((str(file_path), i + 1))

    def _audit_trading_logic(self) -> None:
        """Audit des erreurs logiques sp√©cifiques au trading"""

        # 1. V√©rifier les backtests pour look-ahead bias
        backtest_files = list(SRC_DIR.glob("backtest/**/*.py"))
        for file_path in backtest_files:
            self._check_lookahead_bias(file_path)
            self._check_overfitting_indicators(file_path)

        # 2. V√©rifier les strat√©gies pour risk management
        strategy_files = list(SRC_DIR.glob("strategy/**/*.py"))
        for file_path in strategy_files:
            self._check_risk_controls(file_path)

        # 3. V√©rifier data quality checks
        data_files = list(SRC_DIR.glob("data/**/*.py"))
        for file_path in data_files:
            self._check_data_validation(file_path)

    def _check_lookahead_bias(self, file_path: Path) -> None:
        """D√©tecte le look-ahead bias dans les backtests"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Patterns suspects
            suspicious_patterns = [
                (
                    r"\.iloc\[\s*-?\d+:\s*\]",
                    "Utilisation de iloc sans garantie temporelle",
                ),
                (
                    r"\.sort_values\([^)]*ascending\s*=\s*False",
                    "Tri descendant peut causer look-ahead",
                ),
                (
                    r"\.shift\(\s*-\d+\s*\)",
                    "shift() n√©gatif acc√®de aux donn√©es futures",
                ),
            ]

            for pattern, message in suspicious_patterns:
                for match in re.finditer(pattern, content):
                    line_num = content[: match.start()].count("\n") + 1
                    self._add_finding(
                        category="logic",
                        severity="critical",
                        file_path=str(file_path),
                        line_number=line_num,
                        description=f"Risque de look-ahead bias: {message}",
                        recommendation="V√©rifier que seules les donn√©es pass√©es sont utilis√©es",
                        code_snippet=content[match.start() : match.end()],
                    )
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur check_lookahead_bias pour {file_path}: {e}")

    def _check_overfitting_indicators(self, file_path: Path) -> None:
        """D√©tecte les signes d'overfitting"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Trop de param√®tres optimis√©s
            param_pattern = (
                r"def\s+\w+\([^)]*,\s*[^)]*,\s*[^)]*,\s*[^)]*,\s*[^)]*,\s*[^)]*,"
            )
            if re.search(param_pattern, content):
                self._add_finding(
                    category="logic",
                    severity="high",
                    file_path=str(file_path),
                    line_number=1,
                    description="Fonction avec trop de param√®tres - risque d'overfitting",
                    recommendation="R√©duire le nombre de param√®tres. Utiliser walk-forward ou cross-validation",
                )

            # Absence de validation out-of-sample
            if (
                "backtest" in content.lower()
                and "train_test_split" not in content
                and "walk_forward" not in content
            ):
                self._add_finding(
                    category="logic",
                    severity="high",
                    file_path=str(file_path),
                    line_number=1,
                    description="Backtest sans validation out-of-sample apparente",
                    recommendation="Impl√©menter train/test split ou walk-forward validation",
                )
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur check_overfitting pour {file_path}: {e}")

    def _check_risk_controls(self, file_path: Path) -> None:
        """V√©rifie les contr√¥les de risque dans les strat√©gies"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # V√©rifier pr√©sence de stops
            if (
                "position" in content.lower()
                and "stop" not in content.lower()
                and "risk" not in content.lower()
            ):
                self._add_finding(
                    category="logic",
                    severity="high",
                    file_path=str(file_path),
                    line_number=1,
                    description="Strat√©gie sans contr√¥le de risque apparent (stop-loss, position sizing)",
                    recommendation="Ajouter stop-loss et position sizing bas√© sur volatilit√©",
                )

            # Position sizing hardcod√©
            if re.search(r"position_size\s*=\s*\d+\.?\d*", content):
                match = re.search(r"position_size\s*=\s*\d+\.?\d*", content)
                line_num = content[: match.start()].count("\n") + 1
                self._add_finding(
                    category="logic",
                    severity="medium",
                    file_path=str(file_path),
                    line_number=line_num,
                    description="Position size hardcod√©e - devrait √™tre bas√©e sur volatilit√©",
                    recommendation="Calculer position size avec ATR ou volatilit√©",
                )
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur check_risk_controls pour {file_path}: {e}")

    def _check_data_validation(self, file_path: Path) -> None:
        """V√©rifie la validation des donn√©es"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Manque de validation NaN
            if "pd.read" in content or "DataFrame" in content:
                if (
                    ".isnull()" not in content
                    and ".isna()" not in content
                    and ".dropna()" not in content
                ):
                    self._add_finding(
                        category="logic",
                        severity="medium",
                        file_path=str(file_path),
                        line_number=1,
                        description="Chargement de donn√©es sans v√©rification de valeurs manquantes",
                        recommendation="Ajouter: assert not df.isnull().any().any() ou df.fillna()",
                    )

                # Manque de v√©rification de duplicates
                if ".duplicated()" not in content:
                    self._add_finding(
                        category="logic",
                        severity="low",
                        file_path=str(file_path),
                        line_number=1,
                        description="Pas de v√©rification de timestamps dupliqu√©s",
                        recommendation="Ajouter: assert not df.duplicated().any()",
                    )
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur check_data_validation pour {file_path}: {e}")

    def _detect_code_duplication(self) -> None:
        """D√©tecte la duplication de code"""

        # 1. Blocs de code dupliqu√©s
        for block_hash, locations in self.code_blocks.items():
            if len(locations) > 1:
                self.stats.duplicated_lines += len(locations) * 5

                for file_path, line_num in locations:
                    self._add_finding(
                        category="duplication",
                        severity="medium",
                        file_path=file_path,
                        line_number=line_num,
                        description=f"Bloc de code dupliqu√© ({len(locations)} occurrences)",
                        recommendation="Extraire dans une fonction r√©utilisable",
                    )

        # 2. Imports dupliqu√©s
        import_groups = defaultdict(list)
        for file_path, imports in self.imports_by_file.items():
            import_key = frozenset(imports)
            import_groups[import_key].append(file_path)

        for imports, files in import_groups.items():
            if len(files) > 3 and len(imports) > 5:
                for file_path in files:
                    self._add_finding(
                        category="duplication",
                        severity="low",
                        file_path=file_path,
                        line_number=1,
                        description=f"Groupe d'imports dupliqu√© dans {len(files)} fichiers",
                        recommendation="Cr√©er un module utils/common_imports.py",
                    )

        # Calcul du pourcentage de duplication
        if self.stats.total_lines > 0:
            self.stats.duplication_percentage = (
                self.stats.duplicated_lines / self.stats.total_lines
            ) * 100

    def _audit_structure(self) -> None:
        """Audit de la structure architecturale"""

        # 1. V√©rifier s√©paration UI/Bridge/Engine
        self._check_layer_separation()

        # 2. V√©rifier d√©pendances circulaires
        self._check_circular_dependencies()

        # 3. V√©rifier coh√©sion des modules
        self._check_module_cohesion()

    def _check_layer_separation(self) -> None:
        """V√©rifie la s√©paration des couches UI/Bridge/Engine"""
        ui_files = list(SRC_DIR.glob("ui/**/*.py"))

        for ui_file in ui_files:
            try:
                with open(ui_file, "r", encoding="utf-8") as f:
                    content = f.read()

                # UI ne devrait pas importer directement Engine
                engine_imports = [
                    "from threadx.backtest.engine import",
                    "from threadx.indicators.engine import",
                    "from threadx.optimization.engine import",
                ]

                for pattern in engine_imports:
                    if pattern in content:
                        self._add_finding(
                            category="structural",
                            severity="high",
                            file_path=str(ui_file),
                            line_number=1,
                            description="UI importe directement Engine - violation de l'architecture 3-tier",
                            recommendation="Utiliser Bridge comme interface: from threadx.bridge import ...",
                        )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur check_layer_separation pour {ui_file}: {e}")

    def _check_circular_dependencies(self) -> None:
        """D√©tecte les d√©pendances circulaires"""
        # Impl√©mentation simplifi√©e - peut √™tre √©tendue avec networkx
        pass

    def _check_module_cohesion(self) -> None:
        """V√©rifie la coh√©sion des modules"""
        # V√©rifier que les fichiers __init__.py exportent correctement
        init_files = list(SRC_DIR.rglob("__init__.py"))

        for init_file in init_files:
            try:
                with open(init_file, "r", encoding="utf-8") as f:
                    content = f.read()

                if len(content.strip()) == 0:
                    self._add_finding(
                        category="structural",
                        severity="low",
                        file_path=str(init_file),
                        line_number=1,
                        description="Fichier __init__.py vide - exports non d√©finis",
                        recommendation="D√©finir __all__ pour exports explicites",
                    )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur check_module_cohesion pour {init_file}: {e}")

    def _audit_security(self) -> None:
        """Audit de s√©curit√© basique"""
        python_files = self._collect_python_files()

        for file_path in python_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # Patterns de s√©curit√©
                security_patterns = [
                    (
                        r"eval\s*\(",
                        "critical",
                        "Utilisation de eval() - injection de code possible",
                    ),
                    (
                        r"exec\s*\(",
                        "critical",
                        "Utilisation de exec() - injection de code possible",
                    ),
                    (
                        r"pickle\.loads?\(",
                        "high",
                        "Pickle non s√©curis√© - risque de d√©s√©rialisation",
                    ),
                    (
                        r"os\.system\(",
                        "high",
                        "os.system() - pr√©f√©rer subprocess avec shell=False",
                    ),
                    (
                        r"shell\s*=\s*True",
                        "medium",
                        "subprocess avec shell=True - risque d'injection",
                    ),
                ]

                for pattern, severity, message in security_patterns:
                    for match in re.finditer(pattern, content):
                        line_num = content[: match.start()].count("\n") + 1
                        self._add_finding(
                            category="security",
                            severity=severity,
                            file_path=str(file_path),
                            line_number=line_num,
                            description=message,
                            recommendation="Utiliser des alternatives s√©curis√©es",
                        )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur audit_security pour {file_path}: {e}")

    def _audit_gpu_performance(self) -> None:
        """Audit des performances GPU"""
        gpu_files = list(SRC_DIR.glob("**/gpu*.py")) + list(
            SRC_DIR.glob("indicators/**/*.py")
        )

        for file_path in gpu_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # V√©rifier fallback CPU
                if "cupy" in content or "cp." in content:
                    if "try:" not in content or "except ImportError" not in content:
                        self._add_finding(
                            category="performance",
                            severity="medium",
                            file_path=str(file_path),
                            line_number=1,
                            description="Code GPU sans fallback CPU",
                            recommendation="Ajouter try/except ImportError avec fallback numpy",
                        )

                    # V√©rifier shape checks
                    if "vector_checks" not in content and ".shape" not in content:
                        self._add_finding(
                            category="performance",
                            severity="low",
                            file_path=str(file_path),
                            line_number=1,
                            description="Op√©rations GPU sans v√©rification de shape",
                            recommendation="Utiliser utils/gpu/vector_checks.py",
                        )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur audit_gpu_performance pour {file_path}: {e}")

    def _add_finding(
        self,
        category: str,
        severity: str,
        file_path: str,
        line_number: int,
        description: str,
        recommendation: str,
        code_snippet: str = "",
    ) -> None:
        """Ajoute une d√©couverte d'audit"""
        finding = Finding(
            category=category,
            severity=severity,
            file_path=file_path,
            line_number=line_number,
            description=description,
            recommendation=recommendation,
            code_snippet=code_snippet,
        )
        self.findings.append(finding)
        self.stats.findings_by_severity[severity] += 1
        self.stats.findings_by_category[category] += 1

    def _generate_reports(self) -> None:
        """G√©n√®re les rapports d'audit"""

        # Rapport JSON
        report_data = {
            "stats": {
                "total_files": self.stats.total_files,
                "total_lines": self.stats.total_lines,
                "total_functions": self.stats.total_functions,
                "total_classes": self.stats.total_classes,
                "duplication_percentage": round(self.stats.duplication_percentage, 2),
                "findings_by_severity": dict(self.stats.findings_by_severity),
                "findings_by_category": dict(self.stats.findings_by_category),
            },
            "findings": [
                {
                    "category": f.category,
                    "severity": f.severity,
                    "file": f.file_path,
                    "line": f.line_number,
                    "description": f.description,
                    "recommendation": f.recommendation,
                    "code_snippet": f.code_snippet,
                }
                for f in sorted(
                    self.findings,
                    key=lambda x: (
                        {"critical": 0, "high": 1, "medium": 2, "low": 3}[x.severity],
                        x.file_path,
                    ),
                )
            ],
        }

        with open(REPORT_FILE, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        # Rapport Markdown
        self._generate_markdown_report()

    def _generate_markdown_report(self) -> None:
        """G√©n√®re le rapport Markdown d√©taill√©"""

        with open(MARKDOWN_REPORT, "w", encoding="utf-8") as f:
            f.write("# üîç Rapport d'Audit ThreadX - Analyse Compl√®te\n\n")
            f.write(f"**Date:** {self._get_timestamp()}\n\n")
            f.write("---\n\n")

            # Executive Summary
            f.write("## üìä R√©sum√© Ex√©cutif\n\n")
            f.write(f"- **Fichiers analys√©s:** {self.stats.total_files}\n")
            f.write(f"- **Lignes de code:** {self.stats.total_lines:,}\n")
            f.write(f"- **Fonctions:** {self.stats.total_functions}\n")
            f.write(f"- **Classes:** {self.stats.total_classes}\n")
            f.write(f"- **Duplication:** {self.stats.duplication_percentage:.1f}%\n")
            f.write(f"- **Probl√®mes d√©tect√©s:** {len(self.findings)}\n\n")

            # Score de qualit√©
            quality_score = self._calculate_quality_score()
            f.write(f"### üéØ Score de Qualit√© Global: **{quality_score}/10**\n\n")

            if quality_score >= 8:
                f.write("‚úÖ **Excellente qualit√©** - Continuez ainsi!\n\n")
            elif quality_score >= 6:
                f.write("‚ö†Ô∏è **Qualit√© acceptable** - Am√©liorations recommand√©es\n\n")
            else:
                f.write("üö® **Qualit√© pr√©occupante** - Action imm√©diate requise\n\n")

            # R√©partition par s√©v√©rit√©
            f.write("## üö® R√©partition par S√©v√©rit√©\n\n")
            f.write("| S√©v√©rit√© | Nombre | Pourcentage |\n")
            f.write("|----------|--------|-------------|\n")

            total = len(self.findings)
            for severity in ["critical", "high", "medium", "low"]:
                count = self.stats.findings_by_severity.get(severity, 0)
                pct = (count / total * 100) if total > 0 else 0
                icon = {"critical": "üî¥", "high": "üü†", "medium": "üü°", "low": "üü¢"}[
                    severity
                ]
                f.write(f"| {icon} {severity.capitalize()} | {count} | {pct:.1f}% |\n")
            f.write("\n")

            # R√©partition par cat√©gorie
            f.write("## üìÅ R√©partition par Cat√©gorie\n\n")
            f.write("| Cat√©gorie | Nombre | Description |\n")
            f.write("|-----------|--------|-------------|\n")

            categories = {
                "logic": "Erreurs logiques de trading",
                "duplication": "Duplication de code",
                "structural": "Probl√®mes structurels",
                "security": "Vuln√©rabilit√©s de s√©curit√©",
                "performance": "Probl√®mes de performance",
            }

            for cat, desc in categories.items():
                count = self.stats.findings_by_category.get(cat, 0)
                f.write(f"| {cat.capitalize()} | {count} | {desc} |\n")
            f.write("\n")

            # D√©tails des findings par s√©v√©rit√©
            for severity in ["critical", "high", "medium", "low"]:
                findings = [f for f in self.findings if f.severity == severity]
                if findings:
                    icon = {
                        "critical": "üî¥",
                        "high": "üü†",
                        "medium": "üü°",
                        "low": "üü¢",
                    }[severity]
                    f.write(f"## {icon} Probl√®mes de S√©v√©rit√© {severity.upper()}\n\n")

                    for idx, finding in enumerate(findings, 1):
                        f.write(f"### {idx}. {finding.description}\n\n")
                        f.write(
                            f"**Fichier:** `{finding.file_path}:{finding.line_number}`\n\n"
                        )
                        f.write(f"**Cat√©gorie:** {finding.category}\n\n")
                        f.write(f"**Recommandation:** {finding.recommendation}\n\n")

                        if finding.code_snippet:
                            f.write("**Code concern√©:**\n```python\n")
                            f.write(finding.code_snippet)
                            f.write("\n```\n\n")

                        f.write("---\n\n")

            # Plan d'action
            f.write("## üéØ Plan d'Action Recommand√©\n\n")

            critical = self.stats.findings_by_severity.get("critical", 0)
            high = self.stats.findings_by_severity.get("high", 0)

            if critical > 0:
                f.write(f"### üî¥ URGENT: {critical} probl√®mes critiques\n")
                f.write(
                    "**Action imm√©diate requise** - Ces probl√®mes peuvent causer des pertes financi√®res\n\n"
                )

            if high > 0:
                f.write(f"### üü† PRIORITAIRE: {high} probl√®mes haute priorit√©\n")
                f.write(
                    "**√Ä traiter dans les 48h** - Impact significatif sur la fiabilit√©\n\n"
                )

            if self.stats.duplication_percentage > DUPLICATION_THRESHOLD:
                f.write(
                    f"### ‚ôªÔ∏è REFACTORING: {self.stats.duplication_percentage:.1f}% de duplication\n"
                )
                f.write(f"**Objectif:** R√©duire √† <{DUPLICATION_THRESHOLD}%\n\n")

            # Recommandations g√©n√©rales
            f.write("## üí° Recommandations G√©n√©rales\n\n")
            f.write("1. **Installer les outils d'analyse:**\n")
            f.write("   ```bash\n")
            f.write("   pip install pylint flake8 mypy bandit black\n")
            f.write("   ```\n\n")

            f.write(
                "2. **Configurer pre-commit hooks** pour pr√©venir les r√©gressions\n\n"
            )

            f.write(
                "3. **Augmenter la couverture de tests** pour validation continue\n\n"
            )

            f.write("4. **Impl√©menter CI/CD** avec v√©rifications automatiques\n\n")

            # Footer
            f.write("---\n\n")
            f.write("*G√©n√©r√© par ThreadX Audit System*\n")

    def _calculate_quality_score(self) -> float:
        """Calcule le score de qualit√© global (0-10)"""
        score = 10.0

        # P√©nalit√©s par s√©v√©rit√©
        score -= self.stats.findings_by_severity.get("critical", 0) * 1.0
        score -= self.stats.findings_by_severity.get("high", 0) * 0.5
        score -= self.stats.findings_by_severity.get("medium", 0) * 0.2
        score -= self.stats.findings_by_severity.get("low", 0) * 0.05

        # P√©nalit√© duplication
        if self.stats.duplication_percentage > DUPLICATION_THRESHOLD:
            score -= (self.stats.duplication_percentage - DUPLICATION_THRESHOLD) * 0.1

        return max(0.0, min(10.0, score))

    @staticmethod
    def _get_timestamp() -> str:
        """Retourne timestamp format√©"""
        from datetime import datetime

        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def main():
    """Point d'entr√©e principal"""
    print("=" * 70)
    print("  THREADX AUDIT SYSTEM - Analyse Compl√®te de Qualit√©")
    print("=" * 70)
    print()

    auditor = ThreadXAuditor()

    try:
        auditor.run_full_audit()

        # Afficher r√©sum√©
        print("\n" + "=" * 70)
        print("  R√âSUM√â DE L'AUDIT")
        print("=" * 70)
        print(f"\nüìä Statistiques:")
        print(f"   - Fichiers: {auditor.stats.total_files}")
        print(f"   - Lignes: {auditor.stats.total_lines:,}")
        print(f"   - Duplication: {auditor.stats.duplication_percentage:.1f}%")

        print(f"\nüö® Probl√®mes par s√©v√©rit√©:")
        for severity in ["critical", "high", "medium", "low"]:
            count = auditor.stats.findings_by_severity.get(severity, 0)
            icon = {"critical": "üî¥", "high": "üü†", "medium": "üü°", "low": "üü¢"}[
                severity
            ]
            print(f"   {icon} {severity.capitalize()}: {count}")

        quality_score = auditor._calculate_quality_score()
        print(f"\nüéØ Score de Qualit√©: {quality_score:.1f}/10")

        print("\n" + "=" * 70)

        # Code de sortie bas√© sur s√©v√©rit√©
        if auditor.stats.findings_by_severity.get("critical", 0) > 0:
            sys.exit(2)
        elif auditor.stats.findings_by_severity.get("high", 0) > 0:
            sys.exit(1)
        else:
            sys.exit(0)

    except Exception as e:
        print(f"\n‚ùå Erreur fatale: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(3)


if __name__ == "__main__":
    main()
